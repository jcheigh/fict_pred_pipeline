{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is testing inference with 2/10 generation scheme and then just performing MLE via gradient based optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import beta\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 0.7\n",
    "batch_size = 50\n",
    "pop_size = 1000 # population size per timestep\n",
    "epsilon    = 0.05 # exp prop of neutral words per speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_dgp(rho, N, epsilon):\n",
    "    sigma = 0.175 * (rho ** 2) - 0.3625 * rho + 0.1875\n",
    "    a = rho * ((rho * (1 - rho)) / sigma - 1)\n",
    "    b = (1 - rho) * ((rho * (1 - rho)) / sigma - 1)\n",
    "\n",
    "    print(f'='*10)\n",
    "    print(f'True Alpha: {a}')\n",
    "    print(f\"True Beta: {b}\")\n",
    "\n",
    "    ### u ~ pi Beta(a, b) + (1-pi) Beta(b,a), where pi = 0.5\n",
    "    bmm = tfd.MixtureSameFamily(\n",
    "        mixture_distribution=tfd.Categorical(probs=[0.5, 0.5]),\n",
    "        components_distribution=tfd.Beta(\n",
    "            concentration1=[a, b],\n",
    "            concentration0=[b, a]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Scale transformation bijector: u = 2*u - 1 (Chain goes end to front of list)\n",
    "    scale_transform = tfb.Chain([tfb.Shift(shift=-1.0), tfb.Scale(scale=2.0)])\n",
    "    \n",
    "    transformed_bmm = tfd.TransformedDistribution(\n",
    "        distribution=bmm,\n",
    "        bijector=scale_transform\n",
    "    )\n",
    "    \n",
    "    u = yield tfp.distributions.JointDistributionCoroutineAutoBatched.Root(\n",
    "        tfd.Sample(transformed_bmm, sample_shape=N, name='u')\n",
    "    )\n",
    "\n",
    "    # phi is deterministic given u/epsilon \n",
    "    phi = tf.stack([\n",
    "        (1 - (u + 1) / 2) * (1 - epsilon),\n",
    "        ((u + 1) / 2) * (1 - epsilon),\n",
    "        tf.fill([N], epsilon)\n",
    "    ], axis=-1)\n",
    "    yield tfd.Deterministic(loc=phi, name='phi')\n",
    "\n",
    "    # y = 1(u >= 0)\n",
    "    y = tf.cast(u >= 0, dtype=tf.int32)\n",
    "    yield tfd.Deterministic(loc=y, name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "True Alpha: 6.838461538461533\n",
      "True Beta: 2.930769230769229\n",
      "==========\n",
      "True Alpha: 6.838461538461533\n",
      "True Beta: 2.930769230769229\n"
     ]
    }
   ],
   "source": [
    "true_joint = tfp.distributions.JointDistributionCoroutineAutoBatched(lambda: true_dgp(rho, pop_size, epsilon))\n",
    "true_sample = true_joint.sample()\n",
    "u_samp, phi_samp, y_samp = true_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u sample shape: (1000,)\n",
      "phi sample shape: (1000, 3)\n",
      "y sample shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"u sample shape: {u_samp.shape}\")\n",
    "print(f\"phi sample shape: {phi_samp.shape}\")\n",
    "print(f\"y sample shape: {y_samp.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example u:\n",
      " [ 0.10957897 -0.39057928 -0.3381759   0.10645628 -0.3497519 ]\n",
      "\n",
      "example phi:\n",
      " [[0.42295    0.52705    0.05      ]\n",
      " [0.66052514 0.28947484 0.05      ]\n",
      " [0.6356335  0.31436646 0.05      ]\n",
      " [0.42443326 0.5255667  0.05      ]\n",
      " [0.6411322  0.30886784 0.05      ]]\n",
      "\n",
      "example y:\n",
      " [1 0 0 1 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"example u:\\n {u_samp[:5]}\\n\")\n",
    "print(f\"example phi:\\n {phi_samp[:5]}\\n\")\n",
    "print(f\"example y:\\n {y_samp[:5]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we can only use this for inference\n",
    "dataset = phi_samp, y_samp  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dgp(W, a, b):\n",
    "    ### u ~ pi Beta(a, b) + (1-pi) Beta(b, a), where pi = 0.5\n",
    "    bmm = tfd.MixtureSameFamily(\n",
    "        mixture_distribution=tfd.Categorical(probs=[0.5, 0.5]),\n",
    "        components_distribution=tfd.Beta(\n",
    "            concentration1=[a, b],\n",
    "            concentration0=[b, a]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Scale transformation bijector: u = 2*u - 1 (Chain goes end to front of list)\n",
    "    scale_transform = tfb.Chain([tfb.Shift(shift=-1.0), tfb.Scale(scale=2.0)])\n",
    "    \n",
    "    transformed_bmm = tfd.TransformedDistribution(\n",
    "        distribution=bmm,\n",
    "        bijector=scale_transform,\n",
    "        name='u'\n",
    "    )\n",
    "    \n",
    "    # Define u without specifying sample_shape here\n",
    "    u = yield tfp.distributions.JointDistributionCoroutineAutoBatched.Root(\n",
    "        transformed_bmm\n",
    "    )\n",
    "\n",
    "    phi_values = tf.nn.softmax(W * u)\n",
    "    phi = yield tfp.distributions.JointDistributionCoroutineAutoBatched.Root(\n",
    "        tfd.Deterministic(phi_values, name='phi')\n",
    "    )\n",
    "\n",
    "    # Draw y = 1(u >= 0)\n",
    "    y = tf.cast(u >= 0, dtype=tf.int32)\n",
    "    yield tfp.distributions.JointDistributionCoroutineAutoBatched.Root(\n",
    "        tfd.Deterministic(y, name='y')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainable Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_fit = tfp.util.TransformedVariable(\n",
    "    2., \n",
    "    bijector=tfp.bijectors.Softplus(), \n",
    "    name='alpha_fit'\n",
    "    )\n",
    "alpha_fit = tf.convert_to_tensor(alpha_fit)\n",
    "beta_fit = tfp.util.TransformedVariable(\n",
    "    2.,\n",
    "    bijector=tfp.bijectors.Softplus(),\n",
    "    name='beta_fit'\n",
    "    )\n",
    "beta_fit = tf.convert_to_tensor(beta_fit)\n",
    "w1_fit = tf.Variable(\n",
    "    -3.,\n",
    "    name='w1 (left) fit'\n",
    "    )\n",
    "w2_fit = tf.Variable(\n",
    "    3.,\n",
    "    name='w2 (right) fit'\n",
    "    )\n",
    "w3_fit = tf.Variable(\n",
    "    0.,\n",
    "    name='w3 (right) fit'\n",
    "    )\n",
    "\n",
    "W = tf.stack([w1_fit, w2_fit, w3_fit], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x7f86bc664fe0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "dgp_fit = tfp.distributions.JointDistributionCoroutineAutoBatched(\n",
    "    lambda: dgp(W, alpha_fit, beta_fit)\n",
    ")\n",
    "sample = dgp_fit.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll = lambda : -tf.reduce_sum(dgp_fit.log_prob(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8 ('pymc_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "314398796f4a67be07cfb72fc2092a9f23593aca7769f3685c9d0dfe1f5d1e0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
