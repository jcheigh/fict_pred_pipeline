{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I walk through the generation/inferece pipeline (spending most of the time on inference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math \n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Beta\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "from torch.nn.functional import log_softmax\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9a23a7f350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation:\n",
    "\n",
    "Here I am using 2/10 generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### these params control the generation scheme\n",
    "rho        = 0.8   # polarization\n",
    "pop_size   = 50000 # num individuals\n",
    "epsilon    = 0.05  # expected prop of speech consisting of neutral words\n",
    "pi         = 0.5   # pi == 0.5 => beta mixture symmetrical (choose beta1 with prob pi = 0.5)\n",
    "speech_len = 15    # words per speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Data Generation...\n",
      "====================\n",
      "True Alpha: 2.0000000000000004\n",
      "True Beta: 2.0000000000000004\n",
      "True Epsilon: 0.05\n",
      "\n",
      "mixture samples: [1 1 0 1 1]\n",
      "u samples: [-0.29375126 -0.11814876 -0.29827691 -0.07967257 -0.64165267]\n",
      "\n",
      "y samples: [0 0 0 0 0]\n",
      "\n",
      "phi samples:\n",
      " [[0.61453185 0.33546815 0.05      ]\n",
      " [0.53112066 0.41887934 0.05      ]\n",
      " [0.61668153 0.33331847 0.05      ]\n",
      " [0.51284447 0.43715553 0.05      ]\n",
      " [0.77978502 0.17021498 0.05      ]]\n",
      "\n",
      "X samples:\n",
      " [[11  3  1]\n",
      " [ 8  7  0]\n",
      " [10  3  2]\n",
      " [ 8  7  0]\n",
      " [14  1  0]]\n",
      "\n",
      "====================\n",
      "Generation Time: 2.969 seconds for 50000 samples.\n"
     ]
    }
   ],
   "source": [
    "def generate(rho=rho, N=pop_size, epsilon=epsilon, pi=pi, speech_len=speech_len, verbose=True):\n",
    "    \"\"\"\n",
    "    Uses 2/10 generation scheme to generate N samples.\n",
    "\n",
    "    Returns:\n",
    "        (X, y), (a, b, rho, epsilon, u)\n",
    "        X.size() == [N, 3] is a vector of word counts\n",
    "        y.size() == [N] is a vector of political parties\n",
    "        u.shape  == (N,) is a vector of individual stances\n",
    "        rho is true polarization\n",
    "        epsilon is expected prop of neutral words\n",
    "        a, b are true alpha/beta for beta mixture model\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    if verbose:\n",
    "        print(f'Beginning Data Generation...')\n",
    "        print(f'=' * 20)\n",
    "        \n",
    "    ### get beta mixture model params \n",
    "    sigma = 0.175 * (rho ** 2) - 0.3625 * rho + 0.1875\n",
    "    a     = rho * ((rho * (1 - rho)) / sigma - 1)\n",
    "    b     = (1 - rho) * ((rho * (1 - rho)) / sigma - 1)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"True Alpha: {a}\")\n",
    "        print(f\"True Beta: {b}\")\n",
    "        print(f\"True Epsilon: {epsilon}\\n\")\n",
    "\n",
    "    mean = a / (a + b)\n",
    "    var  = a * b / ((a + b)**2 * (a + b + 1))\n",
    "\n",
    "    if abs(mean - rho) > 10e-15:\n",
    "        print(f'Mean: {mean}')\n",
    "        print(f\"Rho: {rho}\")\n",
    "        raise AssertionError(f\"Mean of BMM params should be rho\")\n",
    "\n",
    "    if abs(var - sigma) > 10e-15:\n",
    "        print(f'Var: {var}')\n",
    "        print(f'Sigma: {sigma}')\n",
    "        raise AssertionError(f\"Var of BMM params should be sigma\")\n",
    "\n",
    "    ### u ~ pi Beta(a, b) + (1 - pi) Beta(b, a)\n",
    "    weights = [pi, 1-pi]\n",
    "    mixture_samples = np.random.choice([0, 1], size=N, p=weights)\n",
    "\n",
    "    u = 2 * np.where(mixture_samples == 1, stats.beta.rvs(a, b, size=N), stats.beta.rvs(b, a, size=N)) - 1\n",
    "\n",
    "    if u.shape != (N,):\n",
    "        raise AssertionError(f\"u.shape should be (N,)\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f'mixture samples: {mixture_samples[:5]}')\n",
    "        print(f'u samples: {u[:5]}\\n')\n",
    "\n",
    "    ### y = 1(u >= 0)\n",
    "    y = (u >= 0).astype(int)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'y samples: {y[:5]}\\n')\n",
    "\n",
    "    ### phi is a prob matrix that is a function of u, epsilon\n",
    "    phi = np.array([(1 - (u+1)/2) * (1 - epsilon), (u+1)/2 * (1 - epsilon), np.repeat(epsilon, N)]).T\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'phi samples:\\n {phi[:5, :]}\\n')\n",
    "\n",
    "    if phi.shape != (N, 3):\n",
    "        raise AssertionError(f'phi.shape should be (N, V) == (N, 3)')\n",
    "\n",
    "    if abs(sum(phi[0]) - 1) > 10e-5:\n",
    "        raise AssertionError(f'rows of phi should sum to 1')\n",
    "    \n",
    "    X = np.array([stats.multinomial.rvs(n=speech_len, p=phi[i, :]) for i in range(N)])\n",
    "\n",
    "    if verbose:\n",
    "        print(f'X samples:\\n {X[:5, :]}\\n')\n",
    "    \n",
    "    if X.shape != (N, 3):\n",
    "        raise AssertionError(f'X.shape should be (N, V) == (N, 3)')\n",
    "\n",
    "    if X[:5].sum() != 15 * 5:\n",
    "        raise AssertionError(f'rows of phi should sum to 1')\n",
    "\n",
    "    X = torch.from_numpy(X).to(torch.float32)\n",
    "    y = torch.from_numpy(y).to(torch.float32)\n",
    "\n",
    "    known   = (X, y)\n",
    "    unknown = (a, b, rho, epsilon, u)\n",
    "\n",
    "    if verbose:\n",
    "        print('=' * 20)\n",
    "        print(f'Generation Time: {round(time.time() - start, 3)} seconds for {N} samples.')\n",
    "\n",
    "    return known, unknown\n",
    "\n",
    "known, unknown = generate()\n",
    "X, y = known\n",
    "a, b, rho, epsilon, u = unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "The goal of inference is to recover $u$ given a sample of $(x,y)$ pairs. We assume the following data generating process:\n",
    "\\begin{align*}\n",
    "    u &\\sim 2 \\cdot \\left(\\pi\\cdot\\mathrm{Beta}\\left(u;\\alpha,\\beta\\right)+(1-\\pi)\\cdot\\mathrm{Beta}\\left(u;\\beta,\\alpha\\right)\\right)-1 \\\\\n",
    "    y &= 1(u \\ge \\lambda) \\\\\n",
    "    x &\\sim \\mathrm{Multinomial}\\left(S, \\mathrm{Softmax}(Wu)\\right), W \\in \\R^V.\n",
    "\\end{align*}\n",
    "\n",
    "For now, we assume $\\lambda = 0, V = 3, \\pi = 0.5$, so our parameters are $\\theta = \\{\\alpha, \\beta, W\\}$. Given we assume this full form of $u$ parameterized by $\\alpha, \\beta$, recovering the distribution of $u$ is equivalent to recovering $\\alpha, \\beta$. This also gives us the following joint:\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "p(u,x^{(n)},y^{(n)};\\theta) &= p(u)p(y^{(n)}|u)\\prod_{s=1}^{S}p(x^{(n)}_s | u) \\\\\n",
    "&= \\left(\\frac{1}{4}\\mathrm{Beta}\\left(\\frac{u+1}{2};\\alpha,\\beta\\right)+\\frac{1}{4}\\mathrm{Beta}\\left(\\frac{u+1}{2};\\beta,\\alpha\\right)\\right) \\\\&\\cdot 1\\left(1(u\\ge0)=y^{(n)}\\right)\\frac{S!}{x^{(n)}_1!x^{(n)}_2!x^{(n)}_3!}\\prod_{s=1}^{S}\\mathrm{softmax}(Wu)_{x^{(n)}_s}\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "Here, we can get the density of $u$ by noting $u$ is a monotonic transform of a random variable whose density is known. Thus, applying a formula found [here](https://en.wikipedia.org/wiki/Probability_density_function#Function_of_random_variables_and_change_of_variables_in_the_probability_density_function) we get the above expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is to maximize the log marginal likelihood. Let $\\theta = \\{\\alpha, \\beta, W\\}$. We'd like to find \n",
    "$$\n",
    "\\theta^* = \\argmax_{\\theta} \\underbrace{\\log p(x^{(1:N)}, y^{(1:N)}; \\theta)}_{\\mathcal{L}(\\theta)}. \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to recover $\\alpha, \\beta$ (hence $u$ given we are assuming $u \\sim 2\\left(\\frac{1}{2} \\cdot \\mathrm{Beta}(\\alpha, \\beta) + \\frac{1}{2} \\cdot \\mathrm{Beta}(\\beta, \\alpha)\\right) - 1$ given $(x,y)$ pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot directly compute this argmax. So, to get MLE estimates for alpha, beta, W, we will take gradient descent steps on the log marginal likelihood. Its gradient is given by the following expression:\n",
    "$$\n",
    "\\nabla_\\theta \\log p(x^{(1:N)},y^{(1:N)}) = \\sum_{n=1}^{N} \\mathbb{E}_{p(u|x^{(n)},y^{(n)};\\theta)}[\\nabla_\\theta \\log p(u,x^{(n)},y^{(n)};\\theta)].\n",
    "$$\n",
    "There are a number of issues here, beginning with the fact that we cannot analytically recover the posterior due to the integral being intractable. However, since $u \\in [-1,1]$, we can discretize the integral, which will be simpler than something like MCMC for sampling/using variational inference to approximate the posterior. To do this, let's write the definition of the expectation:\n",
    "$$\n",
    "\\nabla_\\theta \\log p(x^{(1:N)},y^{(1:N)}) = \\sum_{n=1}^{N} \\int_{-1}^{1} \\frac{p(x^{(n)}, y^{(n)}, u;\\ \\theta)}{p(x^{(n)},y^{(n)};\\  \\theta)}\\nabla_\\theta \\log p(u,x^{(n)},y^{(n)};\\theta) \\mathrm{d}u\n",
    "$$\n",
    "Now, we could assume $u$ takes on $G$ linearly spaced points in $[-1,1]$. One way to do this is to essentially keep track of a hashmap $u \\mapsto p(u, x^{(n)}, y^{(n)})$: \n",
    "$$\n",
    "\\text{Example:}\\  \\{u = -.99 : 0.03, \\dots, u = .99 : 0.007\\}.\n",
    "$$\n",
    "The sum of the values of this hashmap, by the law of total probability, is $p(x^{(n)}, y^{(n)})$. So, the posterior can be obtained by normalizing the values of the hashmap by the sum of the values. However, we can do better. Notice that when $y^{(n)} \\ne 1(u \\ge 0)$, $p(u, x^{(n)}, y^{(n)})$ is necessarily $0$. In other words, half of the entries of our discrete distribution will be $0$ in a very predictable way. Thus, we can just ignore these values and use $y^{(n)}$ as a condition. \n",
    "\n",
    "If $y^{(n)} = 1$, have $u$ take on $G$ linearly spaced points in $[0,1]$, and if $y^{(n)} = 0$ do $[-1,0]$. This is what we do in practice. Theoretically, we're still computing:\n",
    "$$\n",
    "\\nabla_\\theta \\log p(x^{(1:N)},y^{(1:N)}) = \\sum_{n=1}^{N} \\sum_{u} \\frac{p(x^{(n)}, y^{(n)}, u;\\ \\theta)}{p(x^{(n)},y^{(n)};\\  \\theta)}\\nabla_\\theta \\log p(u,x^{(n)},y^{(n)};\\theta).\n",
    "$$\n",
    "It's convenient to move the gradient outside the sum, since then we can efficiently compute the double sum via matrices and then just sum the matrix/call .backward(). This is what I was doing initially, but it's definitely wrong, since the posterior relies on theta. To fix this, I call .detach() on the \"weight\" (posterior probability), which means the gradients only flow through the joint log probability (which is what we want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size  = 200 ### discretize integral into grid_size linearly spaced pts\n",
    "batch_size = 100 ### num samples processed simultaneously\n",
    "num_epochs = 200 ### number epochs\n",
    "lr         = 0.1 ### learning rate\n",
    "\n",
    "def compute_nll(x_batch, y_batch, log_alpha, log_beta, W):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        (x_batch, y_batch) a batch of B (x,y) samples \n",
    "        (log_alpha, log_beta, W) trainable parameters\n",
    "\n",
    "    Returns:\n",
    "        A scaler quantity s.t. calling .backward() will compute\n",
    "        grad_{theta} (L(theta))\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def train(X, y, num_epochs=num_epochs, batch_size=batch_size, grid_size=grid_size, lr=lr, verbose=True):\n",
    "    if verbose:\n",
    "        print(f'Beginning Inference...')\n",
    "        print(f'=' * 20)\n",
    "        print(f'Hyperparameters:')\n",
    "        print(f'Dataset Length: {len(X)}')\n",
    "        print(f'Number Epochs: {num_epochs}')\n",
    "        print(f'Batch Size: {batch_size}')\n",
    "        print(f'Grid Size: {grid_size}')\n",
    "        print(f\"Learning Rate: {lr}\")\n",
    "        print('=' * 20)\n",
    "\n",
    "    ### not strictly necessary \n",
    "    assert len(X) % batch_size == 0\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    dataset    = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    X_batch, y_batch = list(dataloader)[0]\n",
    "\n",
    "    if list(X_batch.size()) != [batch_size, 3]:\n",
    "        raise AssertionError(f\"X_ex.size() should be [batch_size, V] == [batch_size, 3], got {X_batch.size()}\")\n",
    "\n",
    "    if list(y_batch.size()) != [batch_size]:\n",
    "        raise AssertionError(f\"y_ex.size() should be [batch_size], got {y_batch.size()}\")\n",
    "\n",
    "    ### play with parameter initialization!\n",
    "    log_alpha = torch.normal(2, 1, size=(1,), requires_grad=True)\n",
    "    log_beta  = torch.normal(0, 1, size=(1,), requires_grad=True)\n",
    "    W         = torch.normal(0, 2, size=(3,), requires_grad=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Initial Parameters:')\n",
    "        print(f'Alpha: {torch.exp(log_alpha).item()}')\n",
    "        print(f\"Beta: {torch.exp(log_beta).item()}\")\n",
    "        print(f\"W: {W.data.tolist()}\")\n",
    "\n",
    "    optimizer = SGD([log_alpha, log_beta, W], lr=lr)\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch+1} of {num_epochs}')\n",
    "        ### only step after each epoch\n",
    "        optimizer.zero_grad() \n",
    "        nll = 0\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            nll += compute_nll(x_batch, y_batch, log_alpha, log_beta, W, grid_size, batch_size)\n",
    "        \n",
    "        nll = nll / len(dataloader.dataset)\n",
    "        nll.backward()\n",
    "        optimizer.step()\n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch+1}, NLL: {round(nll.item(), 3)}, Alpha: {round(np.exp(log_alpha.item()),3)}, Beta: {round(np.exp(log_beta.item()),3)}, W: {W.tolist()}')\n",
    "            print('=' * 20)\n",
    "\n",
    "    final_alpha = np.exp(log_alpha.item())\n",
    "    final_beta  = np.exp(log_beta.item())\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Training Took {round(time.time() - start, 2)} Seconds.\")\n",
    "        \n",
    "    print(f\"Trained Params: alpha: {final_alpha}, beta: {final_beta}, W: {W.tolist()}\")\n",
    "\n",
    "    return final_alpha, final_beta, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0001! = 6.007541656494141\n",
      "-1! = inf\n"
     ]
    }
   ],
   "source": [
    "### numerically stable factorial\n",
    "factorial = lambda x : torch.exp(torch.lgamma(x+1))\n",
    "\n",
    "assert factorial(torch.tensor(3.)) == torch.tensor(6.)\n",
    "print(f\"3.0001! = {factorial(torch.tensor(3.001)).item()}\")\n",
    "\n",
    "### notice this is infinity, so we can't call factorial on negative stuff\n",
    "print(f\"-1! = {factorial(torch.tensor(-1.)).item()}\")\n",
    "\n",
    "def compute_log_joint(u_mat, x_batch, y_batch, log_alpha, log_beta, W, grid_size, batch_size):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        x_batch (size [batch_size, 3])\n",
    "        y_batch (size [batch_size])\n",
    "        u_mat   (size [batch_size, grid_size])\n",
    "        (log_alpha, log_beta, W) trainable params \n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor (size [batch_size, grid_size]), where the (i,j)th element \n",
    "        is the log joint probability (log p(u, x, y; theta)). Here u is the (i,j)th\n",
    "        element of u_mat, x is the ith row of x_batch, y is the ith element of y_batch\n",
    "\n",
    "    \"\"\"\n",
    "    assert list(u_mat.size())   == [batch_size, grid_size] \n",
    "    assert list(x_batch.size()) == [batch_size, 3]\n",
    "    assert list(y_batch.size()) == [batch_size]\n",
    "\n",
    "    if not torch.all((u_mat >= 0).float() == y_batch.unsqueeze(1)).item():\n",
    "        raise ValueError(f\"u_mat is incompatible with y_batch.\")\n",
    "\n",
    "    ### log prior: log p(u) = log(1/4 Beta((u+1)/2; a, b) + 1/4 Beta((u+1)/2; b, a))\n",
    "    ###                     = log(1/4) + log(Beta((u+1)/2; a, b) + Beta((u+1)/2; b, a))\n",
    "    alpha = torch.exp(log_alpha)\n",
    "    beta  = torch.exp(log_beta)\n",
    "        \n",
    "    beta_dist_ab = Beta(alpha, beta)\n",
    "    beta_dist_ba = Beta(beta, alpha)\n",
    "\n",
    "    ### see 3-6 testing.ipynb test1 to see how log_prob broadcasts\n",
    "    log_beta_prob_ab = beta_dist_ab.log_prob((u_mat + 1) / 2)\n",
    "    log_beta_prob_ba = beta_dist_ba.log_prob((u_mat + 1) / 2)\n",
    "\n",
    "    prior = torch.log(torch.exp(log_beta_prob_ab) + torch.exp(log_beta_prob_ba)) + torch.log(torch.tensor(0.25))\n",
    "\n",
    "    assert list(prior.size()) == [batch_size, grid_size]\n",
    "  \n",
    "    ### log likelihood: log p(x, y | u) = dot(x, log Softmax(Wu)) + log(S!/(x1! x2! x3!)) \n",
    "\n",
    "    ### step 1: compute Wu (see 3-6 testing.ipynb test2 to see this logic)\n",
    "    W_expanded = W.unsqueeze(0).unsqueeze(0)\n",
    "    u_expanded = u_mat.unsqueeze(2)\n",
    "    Wu = torch.matmul(u_expanded, W_expanded) \n",
    "\n",
    "    assert list(W_expanded.size()) == [1,1,3]\n",
    "    assert list(u_expanded.size()) == [batch_size, grid_size, 1]\n",
    "    assert list(Wu.size()) == [batch_size, grid_size, 3]\n",
    "\n",
    "    ### step 2: dot(x, log Softmax(Wu)) (see 3-6 testing.ipynb test3 to see this logic)\n",
    "    likelihood = (x_batch.unsqueeze(1) * log_softmax(Wu, dim=2)).sum(dim=2)\n",
    "\n",
    "    ### ensure not taking negative factorials\n",
    "    assert torch.all(x_batch >= 0)\n",
    "\n",
    "    ### step 3: multinomial constant factors (see 3-6 testing.ipynb test4)\n",
    "    likelihood += torch.log(factorial(torch.tensor(speech_len)))\n",
    "    likelihood -= torch.log(factorial(x_batch)).sum(dim=1, keepdim=True) \n",
    "\n",
    "    assert list(likelihood.size()) == [batch_size, grid_size]\n",
    "    return prior + likelihood\n",
    "\n",
    "\n",
    "def compute_nll(x_batch, y_batch, log_alpha, log_beta, W, grid_size, batch_size):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        x_batch (size [batch_size, 3])\n",
    "        y_batch (size [batch_size])\n",
    "        log_alpha, log_beta, W (trainable params)\n",
    "        grid_size (int) number of lin spaced points to discretize integral into\n",
    "    \n",
    "    Returns:\n",
    "        quantity s.t. .backward() would be the gradient of the nll\n",
    "    \"\"\"\n",
    "    ### see 3-6 testing.ipynb test5 to see this logic\n",
    "    u_mat = torch.empty(batch_size, grid_size)\n",
    "    u_mat[y_batch == 1] = torch.linspace(1/(grid_size+1), 1-1/(grid_size+1), grid_size).repeat((y_batch == 1).sum(), 1)\n",
    "    u_mat[y_batch == 0] = torch.linspace(-1+1/(grid_size+1), -1/(grid_size+1), grid_size).repeat((y_batch == 0).sum(), 1)\n",
    "\n",
    "    assert list(u_mat.size()) == [batch_size, grid_size]\n",
    "\n",
    "    log_joint = compute_log_joint(u_mat, x_batch, y_batch, log_alpha, log_beta, W, grid_size, batch_size)\n",
    "\n",
    "    assert list(log_joint.size()) == [batch_size, grid_size]\n",
    "\n",
    "    ### see 3-6 testing.ipynb test6 to see this logic\n",
    "    max_log_prob = torch.max(log_joint, dim=1, keepdim=True)[0]\n",
    "    joint_probs = torch.exp(log_joint - max_log_prob)\n",
    "    posterior = joint_probs / joint_probs.sum(dim=1, keepdim=True)  \n",
    "\n",
    "    # detach posterior (see 3-6 testing.ipynb test7 to verify this works)\n",
    "    weighted_log_joint = posterior.detach() * log_joint\n",
    "    nll = -weighted_log_joint.sum()\n",
    "\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Testing Inference on Rho = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:35<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Params: alpha: 6.8232979285514315, beta: 4.522689660344638, W: [-2.763429641723633, 1.848551630973816, -0.5217438340187073]\n",
      "Rho Estimate: 0.6013842228444848\n",
      "====================\n",
      "Testing Inference on Rho = 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:40<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Params: alpha: 1.7179977443541095, beta: 1.399244604814195, W: [-2.4644391536712646, 0.4405503571033478, -1.0120877027511597]\n",
      "Rho Estimate: 0.5511274235102316\n",
      "====================\n",
      "Testing Inference on Rho = 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:38<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Params: alpha: 2.170460373265954, beta: 1.4488401813836178, W: [-2.433741331100464, 1.1385084390640259, -0.6374872922897339]\n",
      "Rho Estimate: 0.5996905591268594\n",
      "====================\n",
      "Testing Inference on Rho = 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:35<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Params: alpha: 4.262234093831484, beta: 1.2476877443481504, W: [-1.943561315536499, 1.802489161491394, -0.06956668943166733]\n",
      "Rho Estimate: 0.7735561808331639\n",
      "====================\n",
      "Testing Inference on Rho = 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:37<00:00,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Params: alpha: 13.799174273394556, beta: 3.262731913179484, W: [-1.6382172107696533, 3.7558364868164062, 1.0461071729660034]\n",
      "Rho Estimate: 0.8087709616087962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for rho in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    print(f'=' * 20)\n",
    "    print(f'Testing Inference on Rho = {rho}')\n",
    "    known, unknown = generate(rho=rho, N=10000, verbose=False)\n",
    "    X, y = known\n",
    "    a, b, rho, epsilon, u = unknown\n",
    "\n",
    "    trained_a, trained_b, trained_W = train(X, y, batch_size = 10000, verbose=False)\n",
    "    print(f'Rho Estimate: {(max(trained_a, trained_b))/(trained_a + trained_b)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('fictitious')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3121cab126e9bc214d9c30934815672d7d73cf142023432050561f8ae595f616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
