{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of data generation and simple inference, which is simply gradient based optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math \n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Beta\n",
    "from torch.nn.functional import log_softmax\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = f'/mnt/storage/jcheigh/fictitious-prediction/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Alpha: 12.673684210526261\n",
      "True Beta: 3.1684210526315644\n",
      "True Epsilon: 0.05\n"
     ]
    }
   ],
   "source": [
    "### these params control the generation scheme\n",
    "rho = 0.8\n",
    "pop_size = 10000\n",
    "epsilon = 0.05\n",
    "pi = 0.5\n",
    "speech_len = 15\n",
    "\n",
    "def generate(rho=rho, N=pop_size, epsilon=epsilon, pi=pi, speech_len=15):\n",
    "    ### we specify a mean of one of the modes rho\n",
    "    ### fix a way to get variance sigma from rho\n",
    "    ### then solve the system to get alpha, beta for beta distribution\n",
    "    sigma = 0.175 * (rho ** 2) - 0.3625 * rho + 0.1875\n",
    "    a = rho * ((rho * (1 - rho)) / sigma - 1)\n",
    "    b = (1 - rho) * ((rho * (1 - rho)) / sigma - 1)\n",
    "\n",
    "    ### beta mixture model\n",
    "    weights = [pi, 1-pi]\n",
    "    mixture_samples = np.random.choice([0, 1], size=N, p=weights)\n",
    "    u = 2 * np.where(mixture_samples == 0, stats.beta.rvs(a, b, size=N), stats.beta.rvs(b, a, size=N)) - 1\n",
    "\n",
    "    ### y deterministic given u\n",
    "    y = (u >= 0).astype(int)\n",
    "    \n",
    "    ### left, right, neutral\n",
    "    phi = [(1 - (u+1)/2) * (1 - epsilon), (u+1)/2 * (1 - epsilon), np.repeat(epsilon, N)]\n",
    "    prob_matrix = np.vstack(phi).T \n",
    "    ### x ~ Multinomial(S, phi)\n",
    "    X = np.array([stats.multinomial.rvs(n=speech_len, p=prob_matrix[i, :]) for i in range(N)])\n",
    "\n",
    "    X = torch.from_numpy(X).to(torch.float32)\n",
    "    y = torch.from_numpy(y).to(torch.float32)\n",
    "    known   = (X, y)\n",
    "    unknown = (a, b, rho, epsilon, u)\n",
    "    \n",
    "    return known, unknown\n",
    "\n",
    "\n",
    "known, unknown = generate()\n",
    "\n",
    "a, b, rho, e, u = unknown\n",
    "X, y = known\n",
    "\n",
    "print(f\"True Alpha: {a}\")\n",
    "print(f\"True Beta: {b}\")\n",
    "print(f\"True Epsilon: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y examples:\n",
      " tensor([0., 0., 1., 0., 0.])\n",
      " ==========\n",
      "X examples:\n",
      " tensor([[10.,  5.,  0.],\n",
      "        [11.,  3.,  1.],\n",
      "        [ 1., 11.,  3.],\n",
      "        [13.,  2.,  0.],\n",
      "        [12.,  3.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"y examples:\\n {y[:5]}\\n {'='*10}\")\n",
    "print(f'X examples:\\n {X[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal now is to infer $\\hat{u}^{(i)}$ for each $(x^{(i)}, y^{(i)})$ pair. At a high level, we first need to learn the parameters $\\theta$, which include for example $\\alpha, \\beta$, and then use these parameters to infer $\\hat{u}^{(i)}$. For the first step, we will maximize the log marginal likelihood, and for the second step we will maximize the joint w.r.t $u$ with these trained parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our posterior is intractable, though approximable by discretizing the integral. In other words, we'll assume we can take the gradient but cannot directly argmax. Thus, we'll begin with simple gradient based optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(ABC):\n",
    "    def __init__(\n",
    "        self, \n",
    "        name,\n",
    "        X, \n",
    "        y, \n",
    "        u,\n",
    "        data_dir    = f'{DATA_DIR}/plots',\n",
    "        log_alpha0  = torch.normal(1, 1, size=(1,), requires_grad=True),\n",
    "        log_beta0   = torch.normal(1, 1, size=(1,), requires_grad=True),\n",
    "        W0          = torch.normal(0, 4, size=(3,), requires_grad=True),\n",
    "        num_epochs  = 200,\n",
    "        batch_size  = 100,\n",
    "        approx_size = 25,\n",
    "        grid_size   = 100\n",
    "        ):\n",
    "        self.name = name\n",
    "        self.X          = X\n",
    "        self.y          = y\n",
    "        self.data_dir   = data_dir\n",
    "        self.dataset    = TensorDataset(X, y)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.num_epochs  = num_epochs\n",
    "        self.batch_size  = batch_size\n",
    "        self.approx_size = approx_size\n",
    "        self.grid_size   = grid_size\n",
    "\n",
    "        self.log_alpha   = log_alpha0\n",
    "        self.log_beta    = log_beta0\n",
    "        self.W           = W0\n",
    "\n",
    "        self.log_alpha0  = log_alpha0 \n",
    "        self.log_beta0   = log_beta0 \n",
    "        self.W0          = W0\n",
    "\n",
    "        self.optimizer   = Adam([self.log_alpha, self.log_beta, self.W], lr=0.4)\n",
    "        self.delta       = 1e-5\n",
    "        \n",
    "        ### only used in stance distribution plot\n",
    "        self.true_u      = u \n",
    "        self.history     = []\n",
    "\n",
    "    @abstractmethod\n",
    "    def joint_log_prob(self, u, x_n, y_n):\n",
    "        \"\"\"\n",
    "        u.size() == [batch_size, approx_size]\n",
    "        x_n.size() == [batch_size, 3]\n",
    "        y_n.size() == [batch_size]\n",
    "\n",
    "        Computes log p(u, x_n, y_n; theta) (sometimes averaging sometimes returns a matrix)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def sample_posterior(self, x_n, y_n):\n",
    "        \"\"\"\n",
    "        Samples u ~ p(u | x_n, y_n; theta)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def initialize(self):\n",
    "        self.log_alpha = self.log_alpha0 \n",
    "        self.log_beta  = self.log_beta0 \n",
    "        self.W         = self.W0\n",
    "\n",
    "    def train(self):\n",
    "        self.initialize() \n",
    "        start = time.time()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            total_loss = 0.0\n",
    "            self.optimizer.zero_grad()\n",
    "            print(f'Beginning Epoch {epoch+1} of {self.num_epochs}')\n",
    "  \n",
    "            for x_batch, y_batch in self.dataloader:\n",
    "                u_batch    = self.sample_posterior(x_batch, y_batch) \n",
    "                batch_loss = -self.joint_log_prob(u_batch, x_batch, y_batch)  \n",
    "                batch_loss.backward()\n",
    "                total_loss += batch_loss.item() * x_batch.size(0)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            #epoch_time = time.time() - start\n",
    "            total_loss /= len(self.dataloader.dataset)\n",
    "            # epoch_data = {\n",
    "            #     'epoch': epoch + 1,\n",
    "            #     'loss': total_loss,\n",
    "            #     'time': epoch_time,\n",
    "            #     'alpha': torch.exp(self.log_alpha).item(),\n",
    "            #     'beta': torch.exp(self.log_beta).item(),\n",
    "            #     'W': self.W.data.tolist()\n",
    "            # }\n",
    "            #self.history.append(epoch_data)\n",
    "            print(f'Epoch {epoch+1}: Alpha: {torch.exp(self.log_alpha).item()}, Beta: {torch.exp(self.log_beta).item()}, W: {self.W.data}, Loss: {total_loss}')\n",
    "            print('=' * 20)\n",
    "\n",
    "        final_alpha = torch.exp(self.log_alpha).item()\n",
    "        final_beta = torch.exp(self.log_beta).item()\n",
    "        print(f\"Training Took {round(time.time() - start, 2)} Seconds.\")\n",
    "        print(f\"Trained Params: alpha: {max(final_alpha, final_beta)}, beta: {min(final_alpha, final_beta)}, W: {self.W.data}\")\n",
    "\n",
    "        #self.plot_results()\n",
    "\n",
    "    def plot_results(self):\n",
    "        os.makedirs(f'{self.data_dir}/{self.name}', exist_ok=True)\n",
    "        sns.set(style=\"whitegrid\")  # Set the seaborn style\n",
    "\n",
    "        epochs = [h['epoch'] for h in self.history]\n",
    "        losses = [h['loss'] for h in self.history]\n",
    "        alphas = [h['alpha'] for h in self.history]\n",
    "        betas = [h['beta'] for h in self.history]\n",
    "        Ws = np.array([h['W'] for h in self.history])\n",
    "\n",
    "        # Loss plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(x=epochs, y=losses, label='Loss', marker='o')\n",
    "        plt.title('Loss Convergence')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Negative Log Likelihood')\n",
    "        plt.savefig(f'{self.data_dir}/{self.name}/loss_convergence.jpg')\n",
    "        plt.close()\n",
    "\n",
    "        # Alpha and Beta Convergence\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(x=epochs, y=alphas, label='Alpha', marker='o')\n",
    "        sns.lineplot(x=epochs, y=betas, label='Beta', marker='o')\n",
    "        plt.title('Alpha & Beta Convergence')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'{self.data_dir}/{self.name}/alpha_beta_convergence.jpg')\n",
    "        plt.close()\n",
    "\n",
    "        # W Convergence\n",
    "        for i, w in enumerate(['W1', 'W2', 'W3']):\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.lineplot(x=epochs, y=Ws[:, i], label=w, marker='o')\n",
    "            plt.title(f'{w} Convergence')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Value')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'{self.data_dir}/{self.name}/{w.lower()}_convergence.jpg')\n",
    "            plt.close()\n",
    "\n",
    "            a = torch.exp(self.log_alpha).item()\n",
    "            b = torch.exp(self.log_beta).item()\n",
    "            samples = np.concatenate([\n",
    "                stats.beta.rvs(a, b, size=5000), \n",
    "                stats.beta.rvs(b, a, size=5000)\n",
    "            ]) * 2 - 1\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(self.true_u, bins=30, density=True, alpha=0.5, color='blue', label='Ground Truth')\n",
    "            plt.hist(samples, bins=30, density=True, alpha=0.5, color='red', label='Learned')\n",
    "            plt.title('Stance Distribution')\n",
    "            plt.xlabel('u')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'{self.data_dir}/{self.name}/stance_distribution.jpg')\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Epoch 1 of 200\n",
      "tensor(-128.1144, grad_fn=<UnbindBackward0>)\n",
      "None\n",
      "tensor(-346.1324, grad_fn=<UnbindBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 73\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[39mreturn\u001b[39;00m sampled_u_values\n\u001b[1;32m     72\u001b[0m model \u001b[39m=\u001b[39m VectorGradientDescent(\u001b[39m'\u001b[39m\u001b[39mbasic\u001b[39m\u001b[39m'\u001b[39m, X, y, u, num_epochs\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m model\u001b[39m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[21], line 77\u001b[0m, in \u001b[0;36mGradientDescent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m x_batch, y_batch \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader:\n\u001b[1;32m     76\u001b[0m     u_batch    \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_posterior(x_batch, y_batch) \n\u001b[0;32m---> 77\u001b[0m     batch_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjoint_log_prob(u_batch, x_batch, y_batch)  \n\u001b[1;32m     78\u001b[0m     batch_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     79\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m x_batch\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 38\u001b[0m, in \u001b[0;36mVectorGradientDescent.joint_log_prob\u001b[0;34m(self, u, x_n, y_n, average)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39msum\u001b[39m \u001b[39min\u001b[39;00m sums:\n\u001b[1;32m     37\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39msum\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39msum\u001b[39m\u001b[39m.\u001b[39mbackward())\n\u001b[1;32m     39\u001b[0m \u001b[39m#print(combined_log_prob.backward())\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m combined_log_prob\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m/home/bizon/anaconda3/envs/fictitious/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m/home/bizon/anaconda3/envs/fictitious/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "class VectorGradientDescent(GradientDescent):\n",
    "    \"\"\"\n",
    "    Vectorized gradient descent\n",
    "        - Computes joint log via its closed form\n",
    "        - Computes posterior by discretizing the integral\n",
    "    \"\"\"\n",
    "\n",
    "    def joint_log_prob(self, u, x_n, y_n, average=True):\n",
    "        alpha = torch.exp(self.log_alpha)\n",
    "        beta  = torch.exp(self.log_beta)\n",
    "        \n",
    "        beta_dist_ab = Beta(alpha, beta)\n",
    "        beta_dist_ba = Beta(beta, alpha)\n",
    "\n",
    "        # Compute log probabilities for beta distributions\n",
    "        log_beta_prob_ab = beta_dist_ab.log_prob((u + 1) / 2)\n",
    "        log_beta_prob_ba = beta_dist_ba.log_prob((u + 1) / 2)\n",
    "\n",
    "        # log-sum-exp trick for beta probabilities\n",
    "        max_log_beta_prob = torch.max(log_beta_prob_ab, log_beta_prob_ba)\n",
    "        beta_log_prob = torch.log(torch.exp(log_beta_prob_ab - max_log_beta_prob) + torch.exp(log_beta_prob_ba - max_log_beta_prob)) + max_log_beta_prob\n",
    "\n",
    "        W_expanded = self.W.unsqueeze(0).unsqueeze(0)\n",
    "        u_expanded = u.unsqueeze(2)\n",
    "        softmax_input = torch.matmul(u_expanded, W_expanded)  # Ensure correct squeezing\n",
    "\n",
    "        x_n_log_prob = (x_n.unsqueeze(1) * log_softmax(softmax_input, dim=2)).sum(dim=2)\n",
    "\n",
    "        # Combine log probabilities and average if requested\n",
    "        combined_log_prob = beta_log_prob + x_n_log_prob\n",
    "        \n",
    "        if average:\n",
    "            #print(combined_log_prob)\n",
    "            sums = combined_log_prob.sum(1)\n",
    "            #sums.backward()\n",
    "            for sum in sums:\n",
    "                print(sum)\n",
    "                print(sum.backward())\n",
    "            #print(combined_log_prob.backward())\n",
    "            return combined_log_prob.mean()\n",
    "        else:\n",
    "            return combined_log_prob\n",
    "\n",
    "\n",
    "    def sample_posterior(self, x_n, y_n):\n",
    "        # Create a tensor where each row is linspace(-1, 0, self.grid_size) or linspace(0, 1, self.grid_size)\n",
    "        linspace_neg = torch.linspace(-1 + self.delta, -self.delta, self.grid_size).repeat(len(y_n), 1)\n",
    "        linspace_pos = torch.linspace(self.delta, 1 - self.delta, self.grid_size).repeat(len(y_n), 1)\n",
    "        \n",
    "        # y_n determines which linspace (0 probability o.w.)\n",
    "        u_matrix = torch.where(y_n.unsqueeze(1) == 1, linspace_pos, linspace_neg)\n",
    "\n",
    "        # compute unnormalized log probabilities\n",
    "        log_joint_probs = self.joint_log_prob(u_matrix, x_n, y_n, average=False)  \n",
    "\n",
    "        # log-sum-exp for numerical stability\n",
    "        max_log_prob = torch.max(log_joint_probs, dim=1, keepdim=True)[0] \n",
    "        joint_probs = torch.exp(log_joint_probs - max_log_prob) \n",
    "\n",
    "        # Normalize the probabilities\n",
    "        normalized_probs = joint_probs / joint_probs.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Sample from the multinomial distribution based on normalized probabilities\n",
    "        samples_indices = torch.multinomial(normalized_probs, num_samples=self.approx_size, replacement=True)\n",
    "\n",
    "        # Convert sample indices back to u values\n",
    "        sampled_u_values = torch.gather(u_matrix, 1, samples_indices)\n",
    "\n",
    "        return sampled_u_values\n",
    "\n",
    "\n",
    "model = VectorGradientDescent('basic', X, y, u, num_epochs=200)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n",
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,1,1],[2,2,2]])\n",
    "for a in x.sum(1):\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('fictitious')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3121cab126e9bc214d9c30934815672d7d73cf142023432050561f8ae595f616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
