{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of data generation and simple inference, which is simply gradient based optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import math \n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Beta, Bernoulli\n",
    "from torch.nn.functional import softmax, log_softmax\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Alpha: 12.673684210526261\n",
      "True Beta: 3.1684210526315644\n",
      "True Epsilon: 0.05\n"
     ]
    }
   ],
   "source": [
    "### these params control the generation scheme\n",
    "rho = 0.8\n",
    "pop_size = 5000\n",
    "epsilon = 0.05\n",
    "pi = 0.5\n",
    "speech_len = 15\n",
    "\n",
    "def generate(rho=rho, N=pop_size, epsilon=epsilon, pi=pi, speech_len=15):\n",
    "    ### we specify a mean of one of the modes rho\n",
    "    ### fix a way to get variance sigma from rho\n",
    "    ### then solve the system to get alpha, beta for beta distribution\n",
    "    sigma = 0.175 * (rho ** 2) - 0.3625 * rho + 0.1875\n",
    "    a = rho * ((rho * (1 - rho)) / sigma - 1)\n",
    "    b = (1 - rho) * ((rho * (1 - rho)) / sigma - 1)\n",
    "\n",
    "    ### beta mixture model\n",
    "    weights = [pi, 1-pi]\n",
    "    mixture_samples = np.random.choice([0, 1], size=N, p=weights)\n",
    "    u = 2 * np.where(mixture_samples == 0, stats.beta.rvs(a, b, size=N), stats.beta.rvs(b, a, size=N)) - 1\n",
    "\n",
    "    ### y deterministic given u\n",
    "    y = (u >= 0).astype(int)\n",
    "    \n",
    "    ### left, right, neutral\n",
    "    phi = [(1 - (u+1)/2) * (1 - epsilon), (u+1)/2 * (1 - epsilon), np.repeat(epsilon, N)]\n",
    "    prob_matrix = np.vstack(phi).T \n",
    "    ### x ~ Multinomial(S, phi)\n",
    "    X = np.array([stats.multinomial.rvs(n=speech_len, p=prob_matrix[i, :]) for i in range(N)])\n",
    "    known   = (y, X)\n",
    "    unknown = (a, b, epsilon, u)\n",
    "    \n",
    "    return known, unknown\n",
    "\n",
    "\n",
    "known, unknown = generate()\n",
    "\n",
    "a, b, e, u = unknown\n",
    "y, X = known\n",
    "X = torch.from_numpy(X).to(torch.float32)\n",
    "y = torch.from_numpy(y).to(torch.float32)\n",
    "print(f\"True Alpha: {a}\")\n",
    "print(f\"True Beta: {b}\")\n",
    "print(f\"True Epsilon: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y examples:\n",
      " tensor([0., 1., 0., 0., 0.])\n",
      " ==========\n",
      "X examples:\n",
      " tensor([[12.,  2.,  1.],\n",
      "        [ 0., 15.,  0.],\n",
      "        [10.,  4.,  1.],\n",
      "        [13.,  2.,  0.],\n",
      "        [ 8.,  6.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"y examples:\\n {y[:5]}\\n {'='*10}\")\n",
    "print(f'X examples:\\n {X[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal now is to infer $\\hat{u}^{(i)}$ for each $(x^{(i)}, y^{(i)})$ pair. At a high level, we first need to learn the parameters $\\theta$, which include for example $\\alpha, \\beta$, and then use these parameters to infer $\\hat{u}^{(i)}$. For the first step, we will maximize the log marginal likelihood, and for the second step we will maximize the joint w.r.t $u$ with these trained parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our posterior is intractable, though approximable by discretizing the integral. In other words, we'll assume we can take the gradient but cannot directly argmax. Thus, we'll begin with simple gradient based optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(ABC):\n",
    "    def __init__(\n",
    "        self, \n",
    "        X, \n",
    "        y, \n",
    "        log_alpha0  = torch.tensor(1.5, requires_grad=True),\n",
    "        log_beta0   = torch.tensor(0.7, requires_grad=True),\n",
    "        W0          = torch.tensor([-2.0, 2.0, 0.0], requires_grad=True),\n",
    "        num_epochs  = 200,\n",
    "        batch_size  = 100,\n",
    "        approx_size = 25,\n",
    "        grid_size   = 100\n",
    "        ):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.dataset    = TensorDataset(X, y)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.num_epochs  = num_epochs\n",
    "        self.batch_size  = batch_size\n",
    "        self.approx_size = approx_size\n",
    "        self.grid_size   = grid_size\n",
    "\n",
    "        self.log_alpha   = log_alpha0\n",
    "        self.log_beta    = log_beta0\n",
    "        self.W           = W0\n",
    "\n",
    "        self.log_alpha0  = log_alpha0 \n",
    "        self.log_beta0   = log_beta0 \n",
    "        self.W0          = W0\n",
    "\n",
    "        self.optimizer   = Adam([self.log_alpha, self.log_beta, self.W])\n",
    "        self.delta       = 1e-5\n",
    "\n",
    "    @abstractmethod\n",
    "    def joint_log_prob(self, u, x_n, y_n):\n",
    "        \"\"\"\n",
    "        u.size() == [batch_size, approx_size]\n",
    "        x_n.size() == [batch_size, 3]\n",
    "        y_n.size() == [batch_size]\n",
    "\n",
    "        Computes log p(u, x_n, y_n; theta) (sometimes averaging sometimes returns a matrix)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def sample_posterior(self, x_n, y_n):\n",
    "        \"\"\"\n",
    "        Samples u ~ p(u | x_n, y_n; theta)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def initialize(self):\n",
    "        self.log_alpha = self.log_alpha0 \n",
    "        self.log_beta  = self.log_beta0 \n",
    "        self.W         = self.W0\n",
    "\n",
    "    def train(self):\n",
    "        self.initialize() \n",
    "        start = time.time()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f'Beginning Epoch {epoch+1} of {self.num_epochs}')\n",
    "            total_loss = 0.0\n",
    "            for x_batch, y_batch in self.dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                u_batch    = self.sample_posterior(x_batch, y_batch) \n",
    "                batch_loss = -self.joint_log_prob(u_batch, x_batch, y_batch) \n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += batch_loss.item() * x_batch.size(0)\n",
    "\n",
    "            total_loss /= len(self.dataloader.dataset)\n",
    "            print(f'Epoch {epoch+1}: Alpha: {torch.exp(self.log_alpha).item()}, Beta: {torch.exp(self.log_beta).item()}, W: {self.W.data}, Loss: {total_loss}')\n",
    "            print('=' * 20)\n",
    "\n",
    "        final_alpha = torch.exp(self.log_alpha).item()\n",
    "        final_beta = torch.exp(self.log_beta).item()\n",
    "        print(f\"Training Took {round(time.time() - start, 2)} Seconds.\")\n",
    "        print(f\"Trained Params: alpha: {max(final_alpha, final_beta)}, beta: {min(final_alpha, final_beta)}, W: {self.W.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Epoch 1 of 200\n",
      "Epoch 1: Alpha: 4.683767318725586, Beta: 1.9283682107925415, W: tensor([-2.0358,  2.0345,  0.0038]), Loss: 11.121197681427002\n",
      "====================\n",
      "Beginning Epoch 2 of 200\n",
      "Epoch 2: Alpha: 4.85759973526001, Beta: 1.8635995388031006, W: tensor([-2.0625e+00,  2.0618e+00,  1.5334e-03]), Loss: 11.096861209869385\n",
      "====================\n",
      "Beginning Epoch 3 of 200\n",
      "Epoch 3: Alpha: 4.983427047729492, Beta: 1.8226794004440308, W: tensor([-2.0773e+00,  2.0772e+00,  2.6893e-04]), Loss: 11.083630867004395\n",
      "====================\n",
      "Beginning Epoch 4 of 200\n",
      "Epoch 4: Alpha: 5.070092678070068, Beta: 1.8001948595046997, W: tensor([-2.0847e+00,  2.0865e+00, -1.3506e-03]), Loss: 11.080263710021972\n",
      "====================\n",
      "Beginning Epoch 5 of 200\n",
      "Epoch 5: Alpha: 5.110976219177246, Beta: 1.7926417589187622, W: tensor([-2.0887e+00,  2.0908e+00, -1.7290e-03]), Loss: 11.081938533782958\n",
      "====================\n",
      "Beginning Epoch 6 of 200\n",
      "Epoch 6: Alpha: 5.148494243621826, Beta: 1.7879899740219116, W: tensor([-2.0912e+00,  2.0910e+00, -2.5584e-04]), Loss: 11.081259593963622\n",
      "====================\n",
      "Beginning Epoch 7 of 200\n",
      "Epoch 7: Alpha: 5.175081729888916, Beta: 1.7886967658996582, W: tensor([-2.0889,  2.0958, -0.0050]), Loss: 11.079242267608642\n",
      "====================\n",
      "Beginning Epoch 8 of 200\n",
      "Epoch 8: Alpha: 5.180446624755859, Beta: 1.7970139980316162, W: tensor([-2.0904e+00,  2.0909e+00, -8.5004e-04]), Loss: 11.07967695236206\n",
      "====================\n",
      "Beginning Epoch 9 of 200\n",
      "Epoch 9: Alpha: 5.2032060623168945, Beta: 1.7978683710098267, W: tensor([-2.0904,  2.0934, -0.0025]), Loss: 11.07845344543457\n",
      "====================\n",
      "Beginning Epoch 10 of 200\n",
      "Epoch 10: Alpha: 5.230747699737549, Beta: 1.7974963188171387, W: tensor([-2.0907,  2.0935, -0.0023]), Loss: 11.078645458221436\n",
      "====================\n",
      "Beginning Epoch 11 of 200\n",
      "Epoch 11: Alpha: 5.243754863739014, Beta: 1.8027502298355103, W: tensor([-2.0867,  2.0929, -0.0045]), Loss: 11.079241085052491\n",
      "====================\n",
      "Beginning Epoch 12 of 200\n",
      "Epoch 12: Alpha: 5.2609477043151855, Beta: 1.8057137727737427, W: tensor([-2.0863,  2.0915, -0.0039]), Loss: 11.078548927307128\n",
      "====================\n",
      "Beginning Epoch 13 of 200\n",
      "Epoch 13: Alpha: 5.282919406890869, Beta: 1.8097206354141235, W: tensor([-2.0852e+00,  2.0874e+00, -1.9610e-03]), Loss: 11.077504596710206\n",
      "====================\n",
      "Beginning Epoch 14 of 200\n",
      "Epoch 14: Alpha: 5.298325538635254, Beta: 1.8144986629486084, W: tensor([-2.0831,  2.0899, -0.0049]), Loss: 11.078346900939941\n",
      "====================\n",
      "Beginning Epoch 15 of 200\n",
      "Epoch 15: Alpha: 5.306581974029541, Beta: 1.8216036558151245, W: tensor([-2.0876e+00,  2.0882e+00, -9.8087e-04]), Loss: 11.079450359344483\n",
      "====================\n",
      "Beginning Epoch 16 of 200\n",
      "Epoch 16: Alpha: 5.32631778717041, Beta: 1.8248943090438843, W: tensor([-2.0874e+00,  2.0895e+00, -1.8974e-03]), Loss: 11.077698974609374\n",
      "====================\n",
      "Beginning Epoch 17 of 200\n",
      "Epoch 17: Alpha: 5.334847450256348, Beta: 1.8302608728408813, W: tensor([-2.0869,  2.0913, -0.0033]), Loss: 11.07802131652832\n",
      "====================\n",
      "Beginning Epoch 18 of 200\n",
      "Epoch 18: Alpha: 5.3481550216674805, Beta: 1.8373594284057617, W: tensor([-2.0892e+00,  2.0872e+00,  7.1547e-04]), Loss: 11.080161628723145\n",
      "====================\n",
      "Beginning Epoch 19 of 200\n",
      "Epoch 19: Alpha: 5.362457752227783, Beta: 1.8423691987991333, W: tensor([-2.0848,  2.0951, -0.0069]), Loss: 11.077265453338622\n",
      "====================\n",
      "Beginning Epoch 20 of 200\n",
      "Epoch 20: Alpha: 5.379954814910889, Beta: 1.8447482585906982, W: tensor([-2.0867,  2.0906, -0.0030]), Loss: 11.076800231933595\n",
      "====================\n",
      "Beginning Epoch 21 of 200\n",
      "Epoch 21: Alpha: 5.394587993621826, Beta: 1.849584937095642, W: tensor([-2.0865,  2.0907, -0.0031]), Loss: 11.076193943023682\n",
      "====================\n",
      "Beginning Epoch 22 of 200\n",
      "Epoch 22: Alpha: 5.420140266418457, Beta: 1.853024959564209, W: tensor([-2.0849,  2.0939, -0.0061]), Loss: 11.078929328918457\n",
      "====================\n",
      "Beginning Epoch 23 of 200\n",
      "Epoch 23: Alpha: 5.432444095611572, Beta: 1.8581790924072266, W: tensor([-2.0870,  2.0900, -0.0024]), Loss: 11.07890432357788\n",
      "====================\n",
      "Beginning Epoch 24 of 200\n",
      "Epoch 24: Alpha: 5.44639778137207, Beta: 1.8626471757888794, W: tensor([-2.0865e+00,  2.0863e+00, -3.9529e-04]), Loss: 11.075691757202149\n",
      "====================\n",
      "Beginning Epoch 25 of 200\n",
      "Epoch 25: Alpha: 5.452354431152344, Beta: 1.8689841032028198, W: tensor([-2.0865e+00,  2.0863e+00, -4.0740e-04]), Loss: 11.080362911224364\n",
      "====================\n",
      "Beginning Epoch 26 of 200\n",
      "Epoch 26: Alpha: 5.479646682739258, Beta: 1.8677743673324585, W: tensor([-2.0886,  2.0921, -0.0027]), Loss: 11.077225093841554\n",
      "====================\n",
      "Beginning Epoch 27 of 200\n",
      "Epoch 27: Alpha: 5.477060317993164, Beta: 1.8805294036865234, W: tensor([-2.0897e+00,  2.0900e+00, -7.9171e-04]), Loss: 11.075132942199707\n",
      "====================\n",
      "Beginning Epoch 28 of 200\n",
      "Epoch 28: Alpha: 5.498490810394287, Beta: 1.8817981481552124, W: tensor([-2.0918e+00,  2.0936e+00, -1.8085e-03]), Loss: 11.074156703948974\n",
      "====================\n",
      "Beginning Epoch 29 of 200\n",
      "Epoch 29: Alpha: 5.515419006347656, Beta: 1.886716604232788, W: tensor([-2.0899,  2.0945, -0.0035]), Loss: 11.075763854980469\n",
      "====================\n",
      "Beginning Epoch 30 of 200\n",
      "Epoch 30: Alpha: 5.5335283279418945, Beta: 1.886582374572754, W: tensor([-2.0923,  2.0947, -0.0022]), Loss: 11.07506727218628\n",
      "====================\n",
      "Beginning Epoch 31 of 200\n",
      "Epoch 31: Alpha: 5.5370192527771, Beta: 1.8926588296890259, W: tensor([-2.0915,  2.0946, -0.0026]), Loss: 11.07646448135376\n",
      "====================\n",
      "Beginning Epoch 32 of 200\n",
      "Epoch 32: Alpha: 5.551361560821533, Beta: 1.8957749605178833, W: tensor([-2.0876,  2.0931, -0.0040]), Loss: 11.077352809906007\n",
      "====================\n",
      "Beginning Epoch 33 of 200\n",
      "Epoch 33: Alpha: 5.56414270401001, Beta: 1.8997529745101929, W: tensor([-2.0878,  2.0955, -0.0053]), Loss: 11.074496917724609\n",
      "====================\n",
      "Beginning Epoch 34 of 200\n",
      "Epoch 34: Alpha: 5.5792131423950195, Beta: 1.9053597450256348, W: tensor([-2.0923e+00,  2.0905e+00,  4.8693e-04]), Loss: 11.073919506072999\n",
      "====================\n",
      "Beginning Epoch 35 of 200\n",
      "Epoch 35: Alpha: 5.577657699584961, Beta: 1.9123908281326294, W: tensor([-2.0934,  2.0957, -0.0021]), Loss: 11.072730617523193\n",
      "====================\n",
      "Beginning Epoch 36 of 200\n",
      "Epoch 36: Alpha: 5.591980934143066, Beta: 1.9202206134796143, W: tensor([-2.0908,  2.0968, -0.0043]), Loss: 11.076417541503906\n",
      "====================\n",
      "Beginning Epoch 37 of 200\n",
      "Epoch 37: Alpha: 5.615278244018555, Beta: 1.9235422611236572, W: tensor([-2.0956,  2.0990, -0.0028]), Loss: 11.071520462036133\n",
      "====================\n",
      "Beginning Epoch 38 of 200\n",
      "Epoch 38: Alpha: 5.620371341705322, Beta: 1.9308948516845703, W: tensor([-2.1007e+00,  2.1000e+00, -4.2821e-04]), Loss: 11.073772926330566\n",
      "====================\n",
      "Beginning Epoch 39 of 200\n",
      "Epoch 39: Alpha: 5.626716613769531, Beta: 1.937335729598999, W: tensor([-2.0955,  2.1041, -0.0061]), Loss: 11.073776187896728\n",
      "====================\n",
      "Beginning Epoch 40 of 200\n",
      "Epoch 40: Alpha: 5.649623870849609, Beta: 1.938398838043213, W: tensor([-2.0986,  2.1045, -0.0045]), Loss: 11.069434127807618\n",
      "====================\n",
      "Beginning Epoch 41 of 200\n",
      "Epoch 41: Alpha: 5.6769843101501465, Beta: 1.939469814300537, W: tensor([-2.0976,  2.1040, -0.0047]), Loss: 11.073430519104004\n",
      "====================\n",
      "Beginning Epoch 42 of 200\n",
      "Epoch 42: Alpha: 5.681642055511475, Beta: 1.9451643228530884, W: tensor([-2.1002,  2.1037, -0.0030]), Loss: 11.07219497680664\n",
      "====================\n",
      "Beginning Epoch 43 of 200\n",
      "Epoch 43: Alpha: 5.697488307952881, Beta: 1.9491163492202759, W: tensor([-2.0988,  2.1043, -0.0042]), Loss: 11.072035217285157\n",
      "====================\n",
      "Beginning Epoch 44 of 200\n",
      "Epoch 44: Alpha: 5.7110724449157715, Beta: 1.9530718326568604, W: tensor([-2.1001e+00,  2.1020e+00, -2.0101e-03]), Loss: 11.072913665771484\n",
      "====================\n",
      "Beginning Epoch 45 of 200\n",
      "Epoch 45: Alpha: 5.715084552764893, Beta: 1.9621567726135254, W: tensor([-2.0991e+00,  2.1008e+00, -1.8960e-03]), Loss: 11.073436756134033\n",
      "====================\n",
      "Beginning Epoch 46 of 200\n",
      "Epoch 46: Alpha: 5.718792915344238, Beta: 1.9681252241134644, W: tensor([-2.0995,  2.1057, -0.0047]), Loss: 11.070375595092774\n",
      "====================\n",
      "Beginning Epoch 47 of 200\n",
      "Epoch 47: Alpha: 5.733560562133789, Beta: 1.9746155738830566, W: tensor([-2.1050e+00,  2.1021e+00,  8.7285e-04]), Loss: 11.068080883026123\n",
      "====================\n",
      "Beginning Epoch 48 of 200\n",
      "Epoch 48: Alpha: 5.746492862701416, Beta: 1.9794529676437378, W: tensor([-2.1040,  2.1063, -0.0023]), Loss: 11.068757152557373\n",
      "====================\n",
      "Beginning Epoch 49 of 200\n",
      "Epoch 49: Alpha: 5.770425796508789, Beta: 1.9790855646133423, W: tensor([-2.1043,  2.1070, -0.0026]), Loss: 11.07081615447998\n",
      "====================\n",
      "Beginning Epoch 50 of 200\n",
      "Epoch 50: Alpha: 5.7757697105407715, Beta: 1.9866207838058472, W: tensor([-2.1055,  2.1091, -0.0032]), Loss: 11.068516769409179\n",
      "====================\n",
      "Beginning Epoch 51 of 200\n",
      "Epoch 51: Alpha: 5.7880144119262695, Beta: 1.9910132884979248, W: tensor([-2.1082e+00,  2.1072e+00, -4.0056e-04]), Loss: 11.067756233215333\n",
      "====================\n",
      "Beginning Epoch 52 of 200\n",
      "Epoch 52: Alpha: 5.807829856872559, Beta: 1.9928760528564453, W: tensor([-2.1080e+00,  2.1092e+00, -1.6861e-03]), Loss: 11.071181449890137\n",
      "====================\n",
      "Beginning Epoch 53 of 200\n",
      "Epoch 53: Alpha: 5.82188081741333, Beta: 1.9975266456604004, W: tensor([-2.1082e+00,  2.1065e+00,  3.1537e-05]), Loss: 11.0686301612854\n",
      "====================\n",
      "Beginning Epoch 54 of 200\n",
      "Epoch 54: Alpha: 5.835128307342529, Beta: 2.0038321018218994, W: tensor([-2.1044,  2.1105, -0.0047]), Loss: 11.069459629058837\n",
      "====================\n",
      "Beginning Epoch 55 of 200\n",
      "Epoch 55: Alpha: 5.834717273712158, Beta: 2.0126612186431885, W: tensor([-2.1089,  2.1123, -0.0031]), Loss: 11.070815925598145\n",
      "====================\n",
      "Beginning Epoch 56 of 200\n",
      "Epoch 56: Alpha: 5.838208198547363, Beta: 2.0180673599243164, W: tensor([-2.1131e+00,  2.1124e+00, -6.0668e-04]), Loss: 11.067695732116698\n",
      "====================\n",
      "Beginning Epoch 57 of 200\n",
      "Epoch 57: Alpha: 5.857440948486328, Beta: 2.023019552230835, W: tensor([-2.1137,  2.1157, -0.0022]), Loss: 11.069391899108886\n",
      "====================\n",
      "Beginning Epoch 58 of 200\n",
      "Epoch 58: Alpha: 5.856494903564453, Beta: 2.0328075885772705, W: tensor([-2.1150e+00,  2.1161e+00, -1.7214e-03]), Loss: 11.069341716766358\n",
      "====================\n",
      "Beginning Epoch 59 of 200\n",
      "Epoch 59: Alpha: 5.87705659866333, Beta: 2.0328001976013184, W: tensor([-2.1168,  2.1220, -0.0043]), Loss: 11.067442798614502\n",
      "====================\n",
      "Beginning Epoch 60 of 200\n",
      "Epoch 60: Alpha: 5.8940863609313965, Beta: 2.0405826568603516, W: tensor([-2.1151,  2.1188, -0.0033]), Loss: 11.066899604797364\n",
      "====================\n",
      "Beginning Epoch 61 of 200\n",
      "Epoch 61: Alpha: 5.903470039367676, Beta: 2.046319007873535, W: tensor([-2.1177,  2.1226, -0.0041]), Loss: 11.066719360351563\n",
      "====================\n",
      "Beginning Epoch 62 of 200\n",
      "Epoch 62: Alpha: 5.905623435974121, Beta: 2.0570144653320312, W: tensor([-2.1185,  2.1229, -0.0038]), Loss: 11.064902172088622\n",
      "====================\n",
      "Beginning Epoch 63 of 200\n",
      "Epoch 63: Alpha: 5.928818702697754, Beta: 2.056340217590332, W: tensor([-2.1214,  2.1256, -0.0037]), Loss: 11.066677570343018\n",
      "====================\n",
      "Beginning Epoch 64 of 200\n",
      "Epoch 64: Alpha: 5.932708740234375, Beta: 2.0646426677703857, W: tensor([-2.1237,  2.1264, -0.0029]), Loss: 11.063634815216064\n",
      "====================\n",
      "Beginning Epoch 65 of 200\n",
      "Epoch 65: Alpha: 5.937519073486328, Beta: 2.06746768951416, W: tensor([-2.1258,  2.1300, -0.0038]), Loss: 11.063581104278564\n",
      "====================\n",
      "Beginning Epoch 66 of 200\n",
      "Epoch 66: Alpha: 5.961255073547363, Beta: 2.0708861351013184, W: tensor([-2.1262,  2.1309, -0.0041]), Loss: 11.062920322418213\n",
      "====================\n",
      "Beginning Epoch 67 of 200\n",
      "Epoch 67: Alpha: 5.972475528717041, Beta: 2.0747668743133545, W: tensor([-2.1290,  2.1348, -0.0048]), Loss: 11.06423080444336\n",
      "====================\n",
      "Beginning Epoch 68 of 200\n",
      "Epoch 68: Alpha: 5.975734233856201, Beta: 2.0816259384155273, W: tensor([-2.1276,  2.1337, -0.0050]), Loss: 11.063202648162841\n",
      "====================\n",
      "Beginning Epoch 69 of 200\n",
      "Epoch 69: Alpha: 5.978335857391357, Beta: 2.0890684127807617, W: tensor([-2.1284,  2.1331, -0.0042]), Loss: 11.06425458908081\n",
      "====================\n",
      "Beginning Epoch 70 of 200\n",
      "Epoch 70: Alpha: 6.018359661102295, Beta: 2.085073471069336, W: tensor([-2.1340e+00,  2.1323e+00, -2.6257e-04]), Loss: 11.062144660949707\n",
      "====================\n",
      "Beginning Epoch 71 of 200\n",
      "Epoch 71: Alpha: 6.0053277015686035, Beta: 2.095496654510498, W: tensor([-2.1316,  2.1350, -0.0034]), Loss: 11.064024391174316\n",
      "====================\n",
      "Beginning Epoch 72 of 200\n",
      "Epoch 72: Alpha: 6.011294841766357, Beta: 2.1034903526306152, W: tensor([-2.1336,  2.1354, -0.0024]), Loss: 11.063651275634765\n",
      "====================\n",
      "Beginning Epoch 73 of 200\n",
      "Epoch 73: Alpha: 6.026974201202393, Beta: 2.1071267127990723, W: tensor([-2.1367e+00,  2.1354e+00, -6.1315e-04]), Loss: 11.063937644958497\n",
      "====================\n",
      "Beginning Epoch 74 of 200\n",
      "Epoch 74: Alpha: 6.055802822113037, Beta: 2.106194019317627, W: tensor([-2.1371,  2.1402, -0.0032]), Loss: 11.061184062957764\n",
      "====================\n",
      "Beginning Epoch 75 of 200\n",
      "Epoch 75: Alpha: 6.054263114929199, Beta: 2.1187689304351807, W: tensor([-2.1373e+00,  2.1359e+00, -5.0577e-04]), Loss: 11.06422830581665\n",
      "====================\n",
      "Beginning Epoch 76 of 200\n",
      "Epoch 76: Alpha: 6.074276924133301, Beta: 2.1202104091644287, W: tensor([-2.1377,  2.1416, -0.0037]), Loss: 11.062002964019776\n",
      "====================\n",
      "Beginning Epoch 77 of 200\n",
      "Epoch 77: Alpha: 6.09329080581665, Beta: 2.1236021518707275, W: tensor([-2.1381,  2.1410, -0.0031]), Loss: 11.061420269012451\n",
      "====================\n",
      "Beginning Epoch 78 of 200\n",
      "Epoch 78: Alpha: 6.103880882263184, Beta: 2.1287975311279297, W: tensor([-2.1402e+00,  2.1411e+00, -1.9471e-03]), Loss: 11.061931686401367\n",
      "====================\n",
      "Beginning Epoch 79 of 200\n",
      "Epoch 79: Alpha: 6.121624946594238, Beta: 2.129232168197632, W: tensor([-2.1395,  2.1434, -0.0037]), Loss: 11.06206880569458\n",
      "====================\n",
      "Beginning Epoch 80 of 200\n",
      "Epoch 80: Alpha: 6.122705459594727, Beta: 2.139589548110962, W: tensor([-2.1424e+00,  2.1430e+00, -1.8277e-03]), Loss: 11.062792072296142\n",
      "====================\n",
      "Beginning Epoch 81 of 200\n",
      "Epoch 81: Alpha: 6.131416320800781, Beta: 2.1429131031036377, W: tensor([-2.1385,  2.1426, -0.0039]), Loss: 11.061312236785888\n",
      "====================\n",
      "Beginning Epoch 82 of 200\n",
      "Epoch 82: Alpha: 6.159348964691162, Beta: 2.1421613693237305, W: tensor([-2.1415e+00,  2.1425e+00, -2.0591e-03]), Loss: 11.05803108215332\n",
      "====================\n",
      "Beginning Epoch 83 of 200\n",
      "Epoch 83: Alpha: 6.156533718109131, Beta: 2.1520886421203613, W: tensor([-2.1417e+00,  2.1424e+00, -1.9012e-03]), Loss: 11.061858959197998\n",
      "====================\n",
      "Beginning Epoch 84 of 200\n",
      "Epoch 84: Alpha: 6.1765289306640625, Beta: 2.15248966217041, W: tensor([-2.1395,  2.1422, -0.0031]), Loss: 11.061377944946289\n",
      "====================\n",
      "Beginning Epoch 85 of 200\n",
      "Epoch 85: Alpha: 6.1767778396606445, Beta: 2.158001184463501, W: tensor([-2.1417,  2.1452, -0.0036]), Loss: 11.058928833007812\n",
      "====================\n",
      "Beginning Epoch 86 of 200\n",
      "Epoch 86: Alpha: 6.17749547958374, Beta: 2.1679461002349854, W: tensor([-2.1430,  2.1463, -0.0035]), Loss: 11.058322334289551\n",
      "====================\n",
      "Beginning Epoch 87 of 200\n",
      "Epoch 87: Alpha: 6.188621997833252, Beta: 2.1694750785827637, W: tensor([-2.1446,  2.1487, -0.0040]), Loss: 11.057654609680176\n",
      "====================\n",
      "Beginning Epoch 88 of 200\n",
      "Epoch 88: Alpha: 6.183056831359863, Beta: 2.1776580810546875, W: tensor([-2.1463,  2.1489, -0.0031]), Loss: 11.055363216400146\n",
      "====================\n",
      "Beginning Epoch 89 of 200\n",
      "Epoch 89: Alpha: 6.218439102172852, Beta: 2.172513246536255, W: tensor([-2.1500,  2.1512, -0.0022]), Loss: 11.055229682922363\n",
      "====================\n",
      "Beginning Epoch 90 of 200\n",
      "Epoch 90: Alpha: 6.21433162689209, Beta: 2.182638645172119, W: tensor([-2.1511e+00,  2.1505e+00, -1.1114e-03]), Loss: 11.054999122619629\n",
      "====================\n",
      "Beginning Epoch 91 of 200\n",
      "Epoch 91: Alpha: 6.229132652282715, Beta: 2.1874890327453613, W: tensor([-2.1483,  2.1511, -0.0032]), Loss: 11.058669223785401\n",
      "====================\n",
      "Beginning Epoch 92 of 200\n",
      "Epoch 92: Alpha: 6.241352558135986, Beta: 2.1903536319732666, W: tensor([-2.1494,  2.1515, -0.0028]), Loss: 11.05691987991333\n",
      "====================\n",
      "Beginning Epoch 93 of 200\n",
      "Epoch 93: Alpha: 6.247848033905029, Beta: 2.194587469100952, W: tensor([-2.1509,  2.1534, -0.0031]), Loss: 11.056259689331055\n",
      "====================\n",
      "Beginning Epoch 94 of 200\n",
      "Epoch 94: Alpha: 6.263243675231934, Beta: 2.2006185054779053, W: tensor([-2.1525e+00,  2.1530e+00, -1.8229e-03]), Loss: 11.057429428100585\n",
      "====================\n",
      "Beginning Epoch 95 of 200\n",
      "Epoch 95: Alpha: 6.271188259124756, Beta: 2.2043564319610596, W: tensor([-2.1534,  2.1588, -0.0048]), Loss: 11.055829315185546\n",
      "====================\n",
      "Beginning Epoch 96 of 200\n",
      "Epoch 96: Alpha: 6.295009613037109, Beta: 2.2077322006225586, W: tensor([-2.1542e+00,  2.1539e+00, -1.3296e-03]), Loss: 11.055571365356446\n",
      "====================\n",
      "Beginning Epoch 97 of 200\n",
      "Epoch 97: Alpha: 6.289815902709961, Beta: 2.2166197299957275, W: tensor([-2.1541,  2.1588, -0.0044]), Loss: 11.05598388671875\n",
      "====================\n",
      "Beginning Epoch 98 of 200\n",
      "Epoch 98: Alpha: 6.311147689819336, Beta: 2.220292568206787, W: tensor([-2.1547,  2.1582, -0.0036]), Loss: 11.051881275177003\n",
      "====================\n",
      "Beginning Epoch 99 of 200\n",
      "Epoch 99: Alpha: 6.317543983459473, Beta: 2.2265231609344482, W: tensor([-2.1545,  2.1617, -0.0059]), Loss: 11.054592437744141\n",
      "====================\n",
      "Beginning Epoch 100 of 200\n",
      "Epoch 100: Alpha: 6.320256233215332, Beta: 2.2348904609680176, W: tensor([-2.1578,  2.1621, -0.0041]), Loss: 11.053242321014404\n",
      "====================\n",
      "Beginning Epoch 101 of 200\n",
      "Epoch 101: Alpha: 6.3573150634765625, Beta: 2.2276973724365234, W: tensor([-2.1613e+00,  2.1610e+00, -1.3510e-03]), Loss: 11.051156883239747\n",
      "====================\n",
      "Beginning Epoch 102 of 200\n",
      "Epoch 102: Alpha: 6.330958843231201, Beta: 2.243544101715088, W: tensor([-2.1607,  2.1646, -0.0039]), Loss: 11.050851383209228\n",
      "====================\n",
      "Beginning Epoch 103 of 200\n",
      "Epoch 103: Alpha: 6.362135410308838, Beta: 2.241520404815674, W: tensor([-2.1682e+00,  2.1677e+00, -1.1771e-03]), Loss: 11.046813278198242\n",
      "====================\n",
      "Beginning Epoch 104 of 200\n",
      "Epoch 104: Alpha: 6.377106189727783, Beta: 2.2466442584991455, W: tensor([-2.1663,  2.1696, -0.0035]), Loss: 11.053348121643067\n",
      "====================\n",
      "Beginning Epoch 105 of 200\n",
      "Epoch 105: Alpha: 6.367471694946289, Beta: 2.2595033645629883, W: tensor([-2.1673,  2.1698, -0.0031]), Loss: 11.050457172393799\n",
      "====================\n",
      "Beginning Epoch 106 of 200\n",
      "Epoch 106: Alpha: 6.390288829803467, Beta: 2.264284372329712, W: tensor([-2.1684,  2.1722, -0.0038]), Loss: 11.047306175231933\n",
      "====================\n",
      "Beginning Epoch 107 of 200\n",
      "Epoch 107: Alpha: 6.404479026794434, Beta: 2.266239881515503, W: tensor([-2.1690,  2.1762, -0.0059]), Loss: 11.049073657989503\n",
      "====================\n",
      "Beginning Epoch 108 of 200\n",
      "Epoch 108: Alpha: 6.40814733505249, Beta: 2.2708733081817627, W: tensor([-2.1706,  2.1752, -0.0043]), Loss: 11.054395599365234\n",
      "====================\n",
      "Beginning Epoch 109 of 200\n",
      "Epoch 109: Alpha: 6.418295383453369, Beta: 2.2767105102539062, W: tensor([-2.1739e+00,  2.1720e+00, -3.8470e-04]), Loss: 11.05307502746582\n",
      "====================\n",
      "Beginning Epoch 110 of 200\n",
      "Epoch 110: Alpha: 6.419105052947998, Beta: 2.2854912281036377, W: tensor([-2.1769e+00,  2.1753e+00, -5.2452e-04]), Loss: 11.049214916229248\n",
      "====================\n",
      "Beginning Epoch 111 of 200\n",
      "Epoch 111: Alpha: 6.439328193664551, Beta: 2.2903709411621094, W: tensor([-2.1737,  2.1773, -0.0037]), Loss: 11.048874664306641\n",
      "====================\n",
      "Beginning Epoch 112 of 200\n",
      "Epoch 112: Alpha: 6.450098037719727, Beta: 2.2916274070739746, W: tensor([-2.1754,  2.1776, -0.0029]), Loss: 11.050025367736817\n",
      "====================\n",
      "Beginning Epoch 113 of 200\n",
      "Epoch 113: Alpha: 6.4479570388793945, Beta: 2.2973456382751465, W: tensor([-2.1757,  2.1814, -0.0051]), Loss: 11.047609729766846\n",
      "====================\n",
      "Beginning Epoch 114 of 200\n",
      "Epoch 114: Alpha: 6.462320804595947, Beta: 2.301586627960205, W: tensor([-2.1775,  2.1822, -0.0044]), Loss: 11.045736579895019\n",
      "====================\n",
      "Beginning Epoch 115 of 200\n",
      "Epoch 115: Alpha: 6.474754810333252, Beta: 2.30424427986145, W: tensor([-2.1811e+00,  2.1814e+00, -1.7212e-03]), Loss: 11.05325403213501\n",
      "====================\n",
      "Beginning Epoch 116 of 200\n",
      "Epoch 116: Alpha: 6.504305839538574, Beta: 2.30800461769104, W: tensor([-2.1796,  2.1826, -0.0034]), Loss: 11.04851619720459\n",
      "====================\n",
      "Beginning Epoch 117 of 200\n",
      "Epoch 117: Alpha: 6.511061668395996, Beta: 2.3136825561523438, W: tensor([-2.1791,  2.1809, -0.0026]), Loss: 11.048045902252197\n",
      "====================\n",
      "Beginning Epoch 118 of 200\n",
      "Epoch 118: Alpha: 6.518061637878418, Beta: 2.319283962249756, W: tensor([-2.1799,  2.1820, -0.0028]), Loss: 11.048476848602295\n",
      "====================\n",
      "Beginning Epoch 119 of 200\n",
      "Epoch 119: Alpha: 6.526468753814697, Beta: 2.324784278869629, W: tensor([-2.1792,  2.1844, -0.0047]), Loss: 11.045217380523681\n",
      "====================\n",
      "Beginning Epoch 120 of 200\n",
      "Epoch 120: Alpha: 6.534818649291992, Beta: 2.3322927951812744, W: tensor([-2.1820,  2.1854, -0.0037]), Loss: 11.044458808898925\n",
      "====================\n",
      "Beginning Epoch 121 of 200\n",
      "Epoch 121: Alpha: 6.556663990020752, Beta: 2.335599899291992, W: tensor([-2.1837e+00,  2.1844e+00, -2.0108e-03]), Loss: 11.047197589874267\n",
      "====================\n",
      "Beginning Epoch 122 of 200\n",
      "Epoch 122: Alpha: 6.5501484870910645, Beta: 2.346223831176758, W: tensor([-2.1814,  2.1870, -0.0050]), Loss: 11.04640386581421\n",
      "====================\n",
      "Beginning Epoch 123 of 200\n",
      "Epoch 123: Alpha: 6.56679630279541, Beta: 2.345111608505249, W: tensor([-2.1839,  2.1854, -0.0025]), Loss: 11.046273212432862\n",
      "====================\n",
      "Beginning Epoch 124 of 200\n",
      "Epoch 124: Alpha: 6.595328330993652, Beta: 2.343388795852661, W: tensor([-2.1828,  2.1858, -0.0034]), Loss: 11.045291137695312\n",
      "====================\n",
      "Beginning Epoch 125 of 200\n",
      "Epoch 125: Alpha: 6.610281944274902, Beta: 2.3431410789489746, W: tensor([-2.1858e+00,  2.1858e+00, -1.5626e-03]), Loss: 11.046860961914062\n",
      "====================\n",
      "Beginning Epoch 126 of 200\n",
      "Epoch 126: Alpha: 6.605687141418457, Beta: 2.3552653789520264, W: tensor([-2.1869e+00,  2.1867e+00, -1.4555e-03]), Loss: 11.045766677856445\n",
      "====================\n",
      "Beginning Epoch 127 of 200\n",
      "Epoch 127: Alpha: 6.5966715812683105, Beta: 2.3686912059783936, W: tensor([-2.1861e+00,  2.1855e+00, -1.2206e-03]), Loss: 11.042568950653076\n",
      "====================\n",
      "Beginning Epoch 128 of 200\n",
      "Epoch 128: Alpha: 6.630148410797119, Beta: 2.364537477493286, W: tensor([-2.1899e+00,  2.1877e+00, -2.8490e-04]), Loss: 11.045402221679687\n",
      "====================\n",
      "Beginning Epoch 129 of 200\n",
      "Epoch 129: Alpha: 6.638242244720459, Beta: 2.367724895477295, W: tensor([-2.1892,  2.1905, -0.0023]), Loss: 11.044063224792481\n",
      "====================\n",
      "Beginning Epoch 130 of 200\n",
      "Epoch 130: Alpha: 6.6589131355285645, Beta: 2.370159864425659, W: tensor([-2.1905,  2.1919, -0.0024]), Loss: 11.042520217895508\n",
      "====================\n",
      "Beginning Epoch 131 of 200\n",
      "Epoch 131: Alpha: 6.65929651260376, Beta: 2.3776051998138428, W: tensor([-2.1922,  2.1937, -0.0025]), Loss: 11.045822772979736\n",
      "====================\n",
      "Beginning Epoch 132 of 200\n",
      "Epoch 132: Alpha: 6.672842502593994, Beta: 2.380744457244873, W: tensor([-2.1924,  2.1953, -0.0033]), Loss: 11.043704299926757\n",
      "====================\n",
      "Beginning Epoch 133 of 200\n",
      "Epoch 133: Alpha: 6.679750442504883, Beta: 2.3823888301849365, W: tensor([-2.1948,  2.1975, -0.0032]), Loss: 11.040634880065918\n",
      "====================\n",
      "Beginning Epoch 134 of 200\n",
      "Epoch 134: Alpha: 6.672158241271973, Beta: 2.3914778232574463, W: tensor([-2.1918,  2.1931, -0.0024]), Loss: 11.043512649536133\n",
      "====================\n",
      "Beginning Epoch 135 of 200\n",
      "Epoch 135: Alpha: 6.68994665145874, Beta: 2.3938705921173096, W: tensor([-2.1907,  2.1958, -0.0047]), Loss: 11.046166496276856\n",
      "====================\n",
      "Beginning Epoch 136 of 200\n",
      "Epoch 136: Alpha: 6.689549446105957, Beta: 2.400531053543091, W: tensor([-2.1938e+00,  2.1932e+00, -1.1975e-03]), Loss: 11.04500509262085\n",
      "====================\n",
      "Beginning Epoch 137 of 200\n",
      "Epoch 137: Alpha: 6.733935832977295, Beta: 2.3919968605041504, W: tensor([-2.1934,  2.1949, -0.0025]), Loss: 11.044016551971435\n",
      "====================\n",
      "Beginning Epoch 138 of 200\n",
      "Epoch 138: Alpha: 6.714309215545654, Beta: 2.4051432609558105, W: tensor([-2.1918,  2.1973, -0.0048]), Loss: 11.045021705627441\n",
      "====================\n",
      "Beginning Epoch 139 of 200\n",
      "Epoch 139: Alpha: 6.741682052612305, Beta: 2.399942398071289, W: tensor([-2.1948,  2.1986, -0.0039]), Loss: 11.042695713043212\n",
      "====================\n",
      "Beginning Epoch 140 of 200\n",
      "Epoch 140: Alpha: 6.739890098571777, Beta: 2.4060425758361816, W: tensor([-2.1956,  2.2013, -0.0050]), Loss: 11.043553657531739\n",
      "====================\n",
      "Beginning Epoch 141 of 200\n",
      "Epoch 141: Alpha: 6.744256496429443, Beta: 2.409564256668091, W: tensor([-2.1963,  2.2002, -0.0039]), Loss: 11.038516216278076\n",
      "====================\n",
      "Beginning Epoch 142 of 200\n",
      "Epoch 142: Alpha: 6.731507301330566, Beta: 2.4184672832489014, W: tensor([-2.1971,  2.2044, -0.0060]), Loss: 11.039775238037109\n",
      "====================\n",
      "Beginning Epoch 143 of 200\n",
      "Epoch 143: Alpha: 6.762334823608398, Beta: 2.420588731765747, W: tensor([-2.2005,  2.2030, -0.0031]), Loss: 11.039268054962157\n",
      "====================\n",
      "Beginning Epoch 144 of 200\n",
      "Epoch 144: Alpha: 6.751755714416504, Beta: 2.429739475250244, W: tensor([-2.1984,  2.2045, -0.0053]), Loss: 11.039962253570557\n",
      "====================\n",
      "Beginning Epoch 145 of 200\n",
      "Epoch 145: Alpha: 6.770597457885742, Beta: 2.429079532623291, W: tensor([-2.2008,  2.2058, -0.0046]), Loss: 11.04001802444458\n",
      "====================\n",
      "Beginning Epoch 146 of 200\n",
      "Epoch 146: Alpha: 6.7633466720581055, Beta: 2.438018798828125, W: tensor([-2.2019,  2.2049, -0.0034]), Loss: 11.043496475219726\n",
      "====================\n",
      "Beginning Epoch 147 of 200\n",
      "Epoch 147: Alpha: 6.77510404586792, Beta: 2.439229965209961, W: tensor([-2.2052,  2.2068, -0.0026]), Loss: 11.037053718566895\n",
      "====================\n",
      "Beginning Epoch 148 of 200\n",
      "Epoch 148: Alpha: 6.804294109344482, Beta: 2.438030481338501, W: tensor([-2.2059e+00,  2.2068e+00, -2.1449e-03]), Loss: 11.038571529388427\n",
      "====================\n",
      "Beginning Epoch 149 of 200\n",
      "Epoch 149: Alpha: 6.812411785125732, Beta: 2.437621831893921, W: tensor([-2.2068e+00,  2.2057e+00, -9.4930e-04]), Loss: 11.039858589172363\n",
      "====================\n",
      "Beginning Epoch 150 of 200\n",
      "Epoch 150: Alpha: 6.81226110458374, Beta: 2.44282603263855, W: tensor([-2.2052,  2.2073, -0.0028]), Loss: 11.039977951049805\n",
      "====================\n",
      "Beginning Epoch 151 of 200\n",
      "Epoch 151: Alpha: 6.81618595123291, Beta: 2.447410821914673, W: tensor([-2.2052,  2.2065, -0.0024]), Loss: 11.041822052001953\n",
      "====================\n",
      "Beginning Epoch 152 of 200\n",
      "Epoch 152: Alpha: 6.831283092498779, Beta: 2.448962688446045, W: tensor([-2.2035,  2.2077, -0.0041]), Loss: 11.040757427215576\n",
      "====================\n",
      "Beginning Epoch 153 of 200\n",
      "Epoch 153: Alpha: 6.831800937652588, Beta: 2.4559836387634277, W: tensor([-2.2042,  2.2062, -0.0029]), Loss: 11.038953952789306\n",
      "====================\n",
      "Beginning Epoch 154 of 200\n",
      "Epoch 154: Alpha: 6.855308532714844, Beta: 2.4556162357330322, W: tensor([-2.2029,  2.2070, -0.0041]), Loss: 11.0396555519104\n",
      "====================\n",
      "Beginning Epoch 155 of 200\n",
      "Epoch 155: Alpha: 6.858645439147949, Beta: 2.4620485305786133, W: tensor([-2.2025,  2.2052, -0.0032]), Loss: 11.040155754089355\n",
      "====================\n",
      "Beginning Epoch 156 of 200\n",
      "Epoch 156: Alpha: 6.863173961639404, Beta: 2.4679996967315674, W: tensor([-2.2017,  2.2068, -0.0047]), Loss: 11.03943853378296\n",
      "====================\n",
      "Beginning Epoch 157 of 200\n",
      "Epoch 157: Alpha: 6.884724140167236, Beta: 2.4667677879333496, W: tensor([-2.2081e+00,  2.2053e+00,  6.5084e-05]), Loss: 11.035134944915772\n",
      "====================\n",
      "Beginning Epoch 158 of 200\n",
      "Epoch 158: Alpha: 6.865089416503906, Beta: 2.4807846546173096, W: tensor([-2.2081e+00,  2.2081e+00, -1.5877e-03]), Loss: 11.038427944183349\n",
      "====================\n",
      "Beginning Epoch 159 of 200\n",
      "Epoch 159: Alpha: 6.890036582946777, Beta: 2.4789843559265137, W: tensor([-2.2079,  2.2090, -0.0023]), Loss: 11.039386100769043\n",
      "====================\n",
      "Beginning Epoch 160 of 200\n",
      "Epoch 160: Alpha: 6.888862609863281, Beta: 2.486541986465454, W: tensor([-2.2076e+00,  2.2077e+00, -1.6431e-03]), Loss: 11.037779350280761\n",
      "====================\n",
      "Beginning Epoch 161 of 200\n",
      "Epoch 161: Alpha: 6.89856481552124, Beta: 2.4907045364379883, W: tensor([-2.2080e+00,  2.2089e+00, -2.1564e-03]), Loss: 11.03624792098999\n",
      "====================\n",
      "Beginning Epoch 162 of 200\n",
      "Epoch 162: Alpha: 6.916764259338379, Beta: 2.4904656410217285, W: tensor([-2.2098,  2.2133, -0.0038]), Loss: 11.036622848510742\n",
      "====================\n",
      "Beginning Epoch 163 of 200\n",
      "Epoch 163: Alpha: 6.928563594818115, Beta: 2.493274211883545, W: tensor([-2.2107,  2.2144, -0.0039]), Loss: 11.035126399993896\n",
      "====================\n",
      "Beginning Epoch 164 of 200\n",
      "Epoch 164: Alpha: 6.936497211456299, Beta: 2.497300863265991, W: tensor([-2.2110,  2.2153, -0.0042]), Loss: 11.03744716644287\n",
      "====================\n",
      "Beginning Epoch 165 of 200\n",
      "Epoch 165: Alpha: 6.9216179847717285, Beta: 2.5098719596862793, W: tensor([-2.2116,  2.2148, -0.0036]), Loss: 11.037363719940185\n",
      "====================\n",
      "Beginning Epoch 166 of 200\n",
      "Epoch 166: Alpha: 6.952237606048584, Beta: 2.5068888664245605, W: tensor([-2.2115,  2.2138, -0.0031]), Loss: 11.034485626220704\n",
      "====================\n",
      "Beginning Epoch 167 of 200\n",
      "Epoch 167: Alpha: 6.966414928436279, Beta: 2.509554386138916, W: tensor([-2.2133,  2.2157, -0.0031]), Loss: 11.035963592529297\n",
      "====================\n",
      "Beginning Epoch 168 of 200\n",
      "Epoch 168: Alpha: 6.9760422706604, Beta: 2.5144619941711426, W: tensor([-2.2113,  2.2231, -0.0088]), Loss: 11.03476385116577\n",
      "====================\n",
      "Beginning Epoch 169 of 200\n",
      "Epoch 169: Alpha: 6.969541549682617, Beta: 2.5201292037963867, W: tensor([-2.2187e+00,  2.2188e+00, -1.7666e-03]), Loss: 11.034668159484863\n",
      "====================\n",
      "Beginning Epoch 170 of 200\n",
      "Epoch 170: Alpha: 6.976923942565918, Beta: 2.5268237590789795, W: tensor([-2.2184,  2.2222, -0.0040]), Loss: 11.03413740158081\n",
      "====================\n",
      "Beginning Epoch 171 of 200\n",
      "Epoch 171: Alpha: 6.987682819366455, Beta: 2.530924081802368, W: tensor([-2.2195e+00,  2.2169e+00, -1.4767e-04]), Loss: 11.035405540466309\n",
      "====================\n",
      "Beginning Epoch 172 of 200\n",
      "Epoch 172: Alpha: 6.985212802886963, Beta: 2.534142255783081, W: tensor([-2.2185,  2.2229, -0.0043]), Loss: 11.03572214126587\n",
      "====================\n",
      "Beginning Epoch 173 of 200\n",
      "Epoch 173: Alpha: 6.999105930328369, Beta: 2.536696195602417, W: tensor([-2.2231e+00,  2.2207e+00, -3.0057e-04]), Loss: 11.030750274658203\n",
      "====================\n",
      "Beginning Epoch 174 of 200\n",
      "Epoch 174: Alpha: 7.003059387207031, Beta: 2.5450406074523926, W: tensor([-2.2197,  2.2264, -0.0057]), Loss: 11.03261281967163\n",
      "====================\n",
      "Beginning Epoch 175 of 200\n",
      "Epoch 175: Alpha: 7.022784233093262, Beta: 2.544478416442871, W: tensor([-2.2222,  2.2283, -0.0054]), Loss: 11.034695129394532\n",
      "====================\n",
      "Beginning Epoch 176 of 200\n",
      "Epoch 176: Alpha: 7.037426471710205, Beta: 2.5506114959716797, W: tensor([-2.2235e+00,  2.2227e+00, -1.2140e-03]), Loss: 11.032870903015137\n",
      "====================\n",
      "Beginning Epoch 177 of 200\n",
      "Epoch 177: Alpha: 7.049342632293701, Beta: 2.55102801322937, W: tensor([-2.2253e+00,  2.2257e+00, -2.0178e-03]), Loss: 11.032993640899658\n",
      "====================\n",
      "Beginning Epoch 178 of 200\n",
      "Epoch 178: Alpha: 7.051682472229004, Beta: 2.5534138679504395, W: tensor([-2.2223,  2.2315, -0.0072]), Loss: 11.03611276626587\n",
      "====================\n",
      "Beginning Epoch 179 of 200\n",
      "Epoch 179: Alpha: 7.0364460945129395, Beta: 2.5683696269989014, W: tensor([-2.2296e+00,  2.2265e+00,  9.6278e-05]), Loss: 11.031691341400146\n",
      "====================\n",
      "Beginning Epoch 180 of 200\n",
      "Epoch 180: Alpha: 7.04600715637207, Beta: 2.5714762210845947, W: tensor([-2.2284,  2.2307, -0.0032]), Loss: 11.028113212585449\n",
      "====================\n",
      "Beginning Epoch 181 of 200\n",
      "Epoch 181: Alpha: 7.081728935241699, Beta: 2.566335439682007, W: tensor([-2.2292,  2.2324, -0.0038]), Loss: 11.03113552093506\n",
      "====================\n",
      "Beginning Epoch 182 of 200\n",
      "Epoch 182: Alpha: 7.06834602355957, Beta: 2.5760860443115234, W: tensor([-2.2316e+00,  2.2310e+00, -1.4983e-03]), Loss: 11.031086044311524\n",
      "====================\n",
      "Beginning Epoch 183 of 200\n",
      "Epoch 183: Alpha: 7.078725337982178, Beta: 2.580704689025879, W: tensor([-2.2308,  2.2350, -0.0044]), Loss: 11.031555156707764\n",
      "====================\n",
      "Beginning Epoch 184 of 200\n",
      "Epoch 184: Alpha: 7.09050178527832, Beta: 2.58638596534729, W: tensor([-2.2320,  2.2360, -0.0043]), Loss: 11.031352577209473\n",
      "====================\n",
      "Beginning Epoch 185 of 200\n",
      "Epoch 185: Alpha: 7.095996856689453, Beta: 2.5931711196899414, W: tensor([-2.2352e+00,  2.2348e+00, -1.6934e-03]), Loss: 11.031398277282715\n",
      "====================\n",
      "Beginning Epoch 186 of 200\n",
      "Epoch 186: Alpha: 7.097432613372803, Beta: 2.6011834144592285, W: tensor([-2.2375,  2.2384, -0.0025]), Loss: 11.0278328704834\n",
      "====================\n",
      "Beginning Epoch 187 of 200\n",
      "Epoch 187: Alpha: 7.11683464050293, Beta: 2.6002583503723145, W: tensor([-2.2359,  2.2420, -0.0056]), Loss: 11.027628173828125\n",
      "====================\n",
      "Beginning Epoch 188 of 200\n",
      "Epoch 188: Alpha: 7.105995178222656, Beta: 2.610661268234253, W: tensor([-2.2398e+00,  2.2384e+00, -1.1638e-03]), Loss: 11.026843318939209\n",
      "====================\n",
      "Beginning Epoch 189 of 200\n",
      "Epoch 189: Alpha: 7.149082660675049, Beta: 2.59976863861084, W: tensor([-2.2373,  2.2427, -0.0052]), Loss: 11.028893337249755\n",
      "====================\n",
      "Beginning Epoch 190 of 200\n",
      "Epoch 190: Alpha: 7.1492743492126465, Beta: 2.609231948852539, W: tensor([-2.2351,  2.2402, -0.0050]), Loss: 11.029403266906739\n",
      "====================\n",
      "Beginning Epoch 191 of 200\n",
      "Epoch 191: Alpha: 7.163462162017822, Beta: 2.6094069480895996, W: tensor([-2.2376,  2.2394, -0.0030]), Loss: 11.028985805511475\n",
      "====================\n",
      "Beginning Epoch 192 of 200\n",
      "Epoch 192: Alpha: 7.157586097717285, Beta: 2.6189637184143066, W: tensor([-2.2388,  2.2406, -0.0031]), Loss: 11.025867176055907\n",
      "====================\n",
      "Beginning Epoch 193 of 200\n",
      "Epoch 193: Alpha: 7.191331386566162, Beta: 2.615118980407715, W: tensor([-2.2417e+00,  2.2396e+00, -8.1863e-04]), Loss: 11.029135074615478\n",
      "====================\n",
      "Beginning Epoch 194 of 200\n",
      "Epoch 194: Alpha: 7.191461563110352, Beta: 2.6221275329589844, W: tensor([-2.2414e+00,  2.2388e+00, -4.5219e-04]), Loss: 11.02944725036621\n",
      "====================\n",
      "Beginning Epoch 195 of 200\n",
      "Epoch 195: Alpha: 7.203774929046631, Beta: 2.623089551925659, W: tensor([-2.2383,  2.2446, -0.0058]), Loss: 11.027200775146484\n",
      "====================\n",
      "Beginning Epoch 196 of 200\n",
      "Epoch 196: Alpha: 7.21558141708374, Beta: 2.626579523086548, W: tensor([-2.2400,  2.2430, -0.0038]), Loss: 11.027490825653077\n",
      "====================\n",
      "Beginning Epoch 197 of 200\n",
      "Epoch 197: Alpha: 7.212349891662598, Beta: 2.6348867416381836, W: tensor([-2.2411,  2.2437, -0.0036]), Loss: 11.026739063262939\n",
      "====================\n",
      "Beginning Epoch 198 of 200\n",
      "Epoch 198: Alpha: 7.220241546630859, Beta: 2.6360020637512207, W: tensor([-2.2439e+00,  2.2429e+00, -1.5136e-03]), Loss: 11.022878704071045\n",
      "====================\n",
      "Beginning Epoch 199 of 200\n",
      "Epoch 199: Alpha: 7.226851463317871, Beta: 2.638225555419922, W: tensor([-2.2446,  2.2469, -0.0035]), Loss: 11.028204097747803\n",
      "====================\n",
      "Beginning Epoch 200 of 200\n",
      "Epoch 200: Alpha: 7.219040870666504, Beta: 2.6485514640808105, W: tensor([-2.2462,  2.2468, -0.0025]), Loss: 11.024042491912843\n",
      "====================\n",
      "Training Took 93.41 Seconds.\n",
      "Trained Params: alpha: 7.219040870666504, beta: 2.6485514640808105, W: tensor([-2.2462,  2.2468, -0.0025])\n"
     ]
    }
   ],
   "source": [
    "class VectorGradientDescent(GradientDescent):\n",
    "    \"\"\"\n",
    "    Vectorized gradient descent\n",
    "        - Computes joint log via its closed form\n",
    "        - Computes posterior by discretizing the integral\n",
    "    \"\"\"\n",
    "\n",
    "    def joint_log_prob(self, u, x_n, y_n, average=True):\n",
    "        alpha = torch.exp(self.log_alpha)\n",
    "        beta  = torch.exp(self.log_beta)\n",
    "        \n",
    "        beta_dist_ab = Beta(alpha, beta)\n",
    "        beta_dist_ba = Beta(beta, alpha)\n",
    "\n",
    "        # Compute log probabilities for beta distributions\n",
    "        log_beta_prob_ab = beta_dist_ab.log_prob((u + 1) / 2)\n",
    "        log_beta_prob_ba = beta_dist_ba.log_prob((u + 1) / 2)\n",
    "\n",
    "        # log-sum-exp trick for beta probabilities\n",
    "        max_log_beta_prob = torch.max(log_beta_prob_ab, log_beta_prob_ba)\n",
    "        beta_log_prob = torch.log(torch.exp(log_beta_prob_ab - max_log_beta_prob) + torch.exp(log_beta_prob_ba - max_log_beta_prob)) + max_log_beta_prob\n",
    "\n",
    "        W_expanded = self.W.unsqueeze(0).unsqueeze(0)\n",
    "        u_expanded = u.unsqueeze(2)\n",
    "        softmax_input = torch.matmul(u_expanded, W_expanded)  # Ensure correct squeezing\n",
    "\n",
    "        x_n_log_prob = (x_n.unsqueeze(1) * log_softmax(softmax_input, dim=2)).sum(dim=2)\n",
    "\n",
    "        # Combine log probabilities and average if requested\n",
    "        combined_log_prob = beta_log_prob + x_n_log_prob\n",
    "        \n",
    "        if average:\n",
    "            return combined_log_prob.mean()\n",
    "        else:\n",
    "            return combined_log_prob\n",
    "\n",
    "\n",
    "    def sample_posterior(self, x_n, y_n):\n",
    "        # Create a tensor where each row is linspace(-1, 0, self.grid_size) or linspace(0, 1, self.grid_size)\n",
    "        linspace_neg = torch.linspace(-1 + self.delta, -self.delta, self.grid_size).repeat(len(y_n), 1)\n",
    "        linspace_pos = torch.linspace(self.delta, 1 - self.delta, self.grid_size).repeat(len(y_n), 1)\n",
    "        \n",
    "        # y_n determines which linspace (0 probability o.w.)\n",
    "        u_matrix = torch.where(y_n.unsqueeze(1) == 1, linspace_pos, linspace_neg)\n",
    "\n",
    "        # compute unnormalized log probabilities\n",
    "        log_joint_probs = self.joint_log_prob(u_matrix, x_n, y_n, average=False)  \n",
    "\n",
    "        # log-sum-exp for numerical stability\n",
    "        max_log_prob = torch.max(log_joint_probs, dim=1, keepdim=True)[0] \n",
    "        joint_probs = torch.exp(log_joint_probs - max_log_prob) \n",
    "\n",
    "        # Normalize the probabilities\n",
    "        normalized_probs = joint_probs / joint_probs.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Sample from the multinomial distribution based on normalized probabilities\n",
    "        samples_indices = torch.multinomial(normalized_probs, num_samples=self.approx_size, replacement=True)\n",
    "\n",
    "        # Convert sample indices back to u values\n",
    "        sampled_u_values = torch.gather(u_matrix, 1, samples_indices)\n",
    "\n",
    "        return sampled_u_values\n",
    "\n",
    "\n",
    "model = VectorGradientDescent(X, y)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = torch.exp(model.log_alpha).item()\n",
    "b = torch.exp(model.log_beta).item()\n",
    "\n",
    "samples = np.concatenate([\n",
    "    stats.beta.rvs(a, b, size=5000), \n",
    "    stats.beta.rvs(b, a, size=5000)\n",
    "]) * 2 - 1  \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.hist(u, bins=30, density=True, alpha=0.5, color='blue', label='Ground Truth')\n",
    "plt.hist(samples, bins=30, density=True, alpha=0.5, color='red', label='Learned')\n",
    "\n",
    "plt.title('Stance Distribution')\n",
    "plt.xlabel('u')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('fictitious')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3121cab126e9bc214d9c30934815672d7d73cf142023432050561f8ae595f616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
