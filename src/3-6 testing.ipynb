{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a supplement to 3-6 inference, and it mostly just ensures torch is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math \n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Beta\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "from torch.nn.functional import log_softmax\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff6a56b3550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7000, 0.3000, 0.7000],\n",
       "        [0.7000, 0.3000, 0.7000]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test 1- check if .log_prob() broadcasts across a tensor\n",
    "\"\"\"\n",
    "test = torch.tensor(\n",
    "    [\n",
    "    [1., 0., 1.],\n",
    "    [1., 0., 1.]\n",
    "    ]\n",
    ")\n",
    "test_dist = Bernoulli(torch.tensor([0.7]))\n",
    "torch.exp(test_dist.log_prob(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mat :\n",
      " tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "W_exp:\n",
      " tensor([[[1., 2., 3.]]])\n",
      "W_exp size: torch.Size([1, 1, 3])\n",
      "u_exp[:1]:\n",
      " tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]]])\n",
      "u_exp size:\n",
      " torch.Size([2, 5, 1])\n",
      "tensor([[[1., 2., 3.],\n",
      "         [1., 2., 3.],\n",
      "         [1., 2., 3.],\n",
      "         [1., 2., 3.],\n",
      "         [1., 2., 3.]],\n",
      "\n",
      "        [[1., 2., 3.],\n",
      "         [1., 2., 3.],\n",
      "         [1., 2., 3.],\n",
      "         [1., 2., 3.],\n",
      "         [1., 2., 3.]]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test 2- given W (size [3]), u (size [B, G]), get Wu\n",
    "\n",
    "Really, what we want to do is create a [B,G,3] tensor that is \n",
    "W elementwise multiplied with each element of u.\n",
    "\"\"\"\n",
    "B, G = 2, 5\n",
    "W = torch.tensor([1., 2., 3.])\n",
    "u_mat = torch.ones([B, G])\n",
    "\n",
    "print(f'u_mat :\\n {u_mat}')\n",
    "\n",
    "W_exp = W.unsqueeze(0).unsqueeze(0)\n",
    "print(f'W_exp:\\n {W_exp}')\n",
    "print(f'W_exp size: {W_exp.size()}')\n",
    "\n",
    "u_exp = u_mat.unsqueeze(2)\n",
    "print(f'u_exp[:1]:\\n {u_exp[:1]}')\n",
    "print(f'u_exp size:\\n {u_exp.size()}')\n",
    "\n",
    "Wu = torch.matmul(u_exp, W_exp)\n",
    "\n",
    "assert list(Wu.size()) == [B, G, 3]\n",
    "print(Wu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4076, -1.4076, -0.4076],\n",
       "         [-2.4076, -1.4076, -0.4076],\n",
       "         [-2.4076, -1.4076, -0.4076],\n",
       "         [-2.4076, -1.4076, -0.4076],\n",
       "         [-2.4076, -1.4076, -0.4076]],\n",
       "\n",
       "        [[-2.4076, -1.4076, -0.4076],\n",
       "         [-2.4076, -1.4076, -0.4076],\n",
       "         [-2.4076, -1.4076, -0.4076],\n",
       "         [-2.4076, -1.4076, -0.4076],\n",
       "         [-2.4076, -1.4076, -0.4076]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test 3- We now need do compute dot(x, log Softmax(Wu))\n",
    "\n",
    "The softmax should go along dim 2 (over 3 elements), resulting in a [B, G, 3] tensor \n",
    "\"\"\"\n",
    "\n",
    "log_sm = log_softmax(Wu, dim=2)\n",
    "log_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTest 3- dot(x, log Softmax(Wu))\\n\\nNow that we have log softmax, we need to dot x with log_sm\\nLooking above, notice we have 2 x values (batch size is 2)\\nThe first 5x3 matrix should be dotted with x1 = {x11, x12, x13},\\nwhereas the second should be dotted with x2 = {x21, x21, x32}\\n\\nand then we need to sum across dim 2 (the dim with 3 elements)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test 3- dot(x, log Softmax(Wu))\n",
    "\n",
    "Now that we have log softmax, we need to dot x with log_sm\n",
    "Looking above, notice we have 2 x values (batch size is 2)\n",
    "The first 5x3 matrix should be dotted with x1 = {x11, x12, x13},\n",
    "whereas the second should be dotted with x2 = {x21, x21, x32}\n",
    "\n",
    "and then we need to sum across dim 2 (the dim with 3 elements)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 7.,  3.,  5.]],\n",
      "\n",
      "        [[12.,  2.,  1.]]])\n",
      "tensor([[[-16.8532,  -4.2228,  -2.0380],\n",
      "         [-16.8532,  -4.2228,  -2.0380],\n",
      "         [-16.8532,  -4.2228,  -2.0380],\n",
      "         [-16.8532,  -4.2228,  -2.0380],\n",
      "         [-16.8532,  -4.2228,  -2.0380]],\n",
      "\n",
      "        [[-28.8913,  -2.8152,  -0.4076],\n",
      "         [-28.8913,  -2.8152,  -0.4076],\n",
      "         [-28.8913,  -2.8152,  -0.4076],\n",
      "         [-28.8913,  -2.8152,  -0.4076],\n",
      "         [-28.8913,  -2.8152,  -0.4076]]])\n",
      "tensor([[-23.1141, -23.1141, -23.1141, -23.1141, -23.1141],\n",
      "        [-32.1141, -32.1141, -32.1141, -32.1141, -32.1141]])\n"
     ]
    }
   ],
   "source": [
    "x_batch= torch.tensor([\n",
    "    [7., 3., 5.],\n",
    "    [12., 2., 1.]\n",
    "])\n",
    "\n",
    "x_exp = x_batch.unsqueeze(1)\n",
    "print(x_exp)\n",
    "\n",
    "print(x_exp * log_softmax(Wu, dim=2))\n",
    "res = (x_exp * log_softmax(Wu, dim=2)).sum(dim=2)\n",
    "print(res)\n",
    "\n",
    "assert list(res.size()) == [B, G]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log(factorial(x_batch)):\n",
      " tensor([[ 8.5252,  1.7918,  4.7875],\n",
      "        [19.9872,  0.6931,  0.0000]])\n",
      "tensor([[15.1044],\n",
      "        [20.6804]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-10.3192, -10.3192, -10.3192, -10.3192, -10.3192],\n",
       "        [-24.8952, -24.8952, -24.8952, -24.8952, -24.8952]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test 4- adding constants to log likelihood\n",
    "\n",
    "We need to add log(S!), which is easy\n",
    "We also need to - log(x1!) - log(x2!) - log(x3!)\n",
    "\"\"\"\n",
    "factorial = lambda x : torch.exp(torch.lgamma(x+1))\n",
    "\n",
    "### add log(S!)\n",
    "res += torch.log(factorial(torch.tensor(15.)))\n",
    "\n",
    "print(f'log(factorial(x_batch)):\\n {torch.log(factorial(x_batch))}')\n",
    "other = torch.log(factorial(x_batch)).sum(dim=1, keepdim=True)\n",
    "print(other)\n",
    "\n",
    "res -= other\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1250,  0.2500,  0.3750,  0.5000,  0.6250,  0.7500,  0.8750,  1.0000],\n",
       "        [-0.8750, -0.7500, -0.6250, -0.5000, -0.3750, -0.2500, -0.1250,  0.0000],\n",
       "        [ 0.1250,  0.2500,  0.3750,  0.5000,  0.6250,  0.7500,  0.8750,  1.0000],\n",
       "        [ 0.1250,  0.2500,  0.3750,  0.5000,  0.6250,  0.7500,  0.8750,  1.0000],\n",
       "        [-0.8750, -0.7500, -0.6250, -0.5000, -0.3750, -0.2500, -0.1250,  0.0000],\n",
       "        [ 0.1250,  0.2500,  0.3750,  0.5000,  0.6250,  0.7500,  0.8750,  1.0000],\n",
       "        [-0.8750, -0.7500, -0.6250, -0.5000, -0.3750, -0.2500, -0.1250,  0.0000],\n",
       "        [-0.8750, -0.7500, -0.6250, -0.5000, -0.3750, -0.2500, -0.1250,  0.0000],\n",
       "        [ 0.1250,  0.2500,  0.3750,  0.5000,  0.6250,  0.7500,  0.8750,  1.0000],\n",
       "        [-0.8750, -0.7500, -0.6250, -0.5000, -0.3750, -0.2500, -0.1250,  0.0000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test 5- discretizing space depending on y\n",
    "\n",
    "Given y_batch with size [batch_size], create a\n",
    "[batch_size, grid_size] matrix, where each row is\n",
    "grid_size linearly spaced points from\n",
    "    (0, 1) if y_batch[row] == 1\n",
    "    (-1, 0) if y_batch[row] == 0\n",
    "\"\"\"\n",
    "B = 10\n",
    "G = 7\n",
    "u_mat = torch.empty(B, G)\n",
    "\n",
    "y_batch = torch.tensor([1., 0., 1., 1., 0., 1., 0., 0., 1., 0.])\n",
    "\n",
    "u_mat[y_batch == 1] = torch.linspace(1/(G+1), 1-1/(G+1), G).repeat((y_batch == 1).sum(), 1)\n",
    "u_mat[y_batch == 0] = torch.linspace(-1+1/(G+1), -1/(G+1), G).repeat((y_batch == 0).sum(), 1)\n",
    "\n",
    "torch.hstack([u_mat, y_batch.unsqueeze(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Joint:\n",
      " tensor([[-7.6391,  1.9458, -3.7146],\n",
      "        [-0.2663,  2.2826,  3.0587]])\n",
      "Normalized Posterior: \n",
      "tensor([[6.8515e-05, 9.9646e-01, 3.4687e-03],\n",
      "        [2.4044e-02, 3.0758e-01, 6.6838e-01]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test 6- getting posterior from log_joint (size [B, G])\n",
    "\n",
    "First we need to exponentiate (to get joint), and then normalize \n",
    "The resulting matrix is size [B, G], where each row is the posterior\n",
    "for that x^{(n)}, y^{(n)} pair\n",
    "\"\"\"\n",
    "\n",
    "log_joint = torch.normal(0, 5, size=(2, 3))\n",
    "\n",
    "print(f'Log Joint:\\n {log_joint}')\n",
    "print(f'Normalized Posterior: \\n{torch.exp(log_joint) / torch.exp(log_joint).sum(dim=1, keepdim=True)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9458],\n",
      "        [3.0587]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[6.8515e-05, 9.9646e-01, 3.4687e-03],\n",
       "        [2.4044e-02, 3.0758e-01, 6.6837e-01]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### we can also use log sum exp to get the same result\n",
    "max_log_prob = torch.max(log_joint, dim=1, keepdim=True)[0]\n",
    "print(max_log_prob)\n",
    "joint_probs = torch.exp(log_joint - max_log_prob)\n",
    "posterior = joint_probs / joint_probs.sum(dim=1, keepdim=True)  \n",
    "posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=1: With Grad=[-0.88195246 -3.8100348 ], With Detach=[-0.8819524645805359, -3.81003475189209], Without Detach=[-0.2932586669921875, -3.7180514335632324]\n",
      "N=6: With Grad=[-2.1607833 -9.334585 ], With Detach=[-2.160783529281616, -9.334585189819336], Without Detach=[-0.7184838652610779, -9.109225273132324]\n",
      "N=11: With Grad=[ -2.6633883 -11.505837 ], With Detach=[-2.663388252258301, -11.505838394165039], Without Detach=[-0.8856051564216614, -11.228058815002441]\n",
      "N=16: With Grad=[ -2.9816425 -12.880694 ], With Detach=[-2.981642246246338, -12.880696296691895], Without Detach=[-0.9914281964302063, -12.569723129272461]\n",
      "N=21: With Grad=[ -3.2150335 -13.888944 ], With Detach=[-3.2150330543518066, -13.888943672180176], Without Detach=[-1.069032907485962, -13.553629875183105]\n",
      "N=26: With Grad=[ -3.3994153 -14.6854725], With Detach=[-3.3994150161743164, -14.685473442077637], Without Detach=[-1.1303420066833496, -14.33092975616455]\n",
      "N=31: With Grad=[ -3.551839 -15.343944], With Detach=[-3.5518386363983154, -15.343945503234863], Without Detach=[-1.1810245513916016, -14.973502159118652]\n",
      "N=36: With Grad=[ -3.681763 -15.905214], With Detach=[-3.6817626953125, -15.905217170715332], Without Detach=[-1.2242255210876465, -15.521224021911621]\n",
      "N=41: With Grad=[ -3.794983 -16.394323], With Detach=[-3.794982671737671, -16.394325256347656], Without Detach=[-1.2618722915649414, -15.998527526855469]\n",
      "N=46: With Grad=[ -3.8953085 -16.82773  ], With Detach=[-3.895308256149292, -16.82773208618164], Without Detach=[-1.295231819152832, -16.421470642089844]\n",
      "N=51: With Grad=[ -3.9853787 -17.216833 ], With Detach=[-3.9853782653808594, -17.216835021972656], Without Detach=[-1.325181007385254, -16.801177978515625]\n",
      "N=56: With Grad=[ -4.067097 -17.569859], With Detach=[-4.06709623336792, -17.56985855102539], Without Detach=[-1.3523533344268799, -17.145679473876953]\n",
      "N=61: With Grad=[ -4.1418815 -17.892931 ], With Detach=[-4.141880989074707, -17.892929077148438], Without Detach=[-1.3772199153900146, -17.460952758789062]\n",
      "N=66: With Grad=[ -4.210818 -18.190737], With Detach=[-4.210817337036133, -18.19073486328125], Without Detach=[-1.400141954421997, -17.751567840576172]\n",
      "N=71: With Grad=[ -4.274755 -18.466944], With Detach=[-4.274754047393799, -18.466938018798828], Without Detach=[-1.4214017391204834, -18.021102905273438]\n",
      "N=76: With Grad=[ -4.334368 -18.724474], With Detach=[-4.334367275238037, -18.724468231201172], Without Detach=[-1.4412236213684082, -18.272415161132812]\n",
      "N=81: With Grad=[ -4.3902063 -18.965693 ], With Detach=[-4.390204906463623, -18.965686798095703], Without Detach=[-1.4597902297973633, -18.507808685302734]\n",
      "N=86: With Grad=[ -4.4427185 -19.192543 ], With Detach=[-4.4427170753479, -19.19253921508789], Without Detach=[-1.4772510528564453, -18.72918701171875]\n",
      "N=91: With Grad=[ -4.492279 -19.406641], With Detach=[-4.4922776222229, -19.406639099121094], Without Detach=[-1.4937305450439453, -18.93811798095703]\n",
      "N=96: With Grad=[ -4.5392017 -19.60935  ], With Detach=[-4.539200782775879, -19.60934829711914], Without Detach=[-1.5093328952789307, -19.135929107666016]\n"
     ]
    }
   ],
   "source": [
    "### detach (code from ChatGPT)\n",
    "\n",
    "# Correcting the approach to avoid in-place operations and properly manage gradient tracking\n",
    "\n",
    "results_with_grad = []\n",
    "results_with_detach = []\n",
    "results_without_detach = []\n",
    "\n",
    "for N in range(1, 101, 5):\n",
    "    # Reinitialize parameters for each N to clear gradients\n",
    "    theta1 = torch.tensor(3.2, requires_grad=True)\n",
    "    theta2 = torch.tensor(1.2, requires_grad=True)\n",
    "    \n",
    "    gradient_contributions = []\n",
    "    detached_contributions = []\n",
    "    without_detach_contributions = []\n",
    "\n",
    "    for n in range(1, N + 1):\n",
    "        sin_term = torch.sin(theta1.pow(2) + theta2) / n\n",
    "        cos_term = torch.cos(theta1 + theta2.pow(3))\n",
    "        \n",
    "        # Compute gradient contributions\n",
    "        cos_term.backward(retain_graph=True)\n",
    "        grad_with_grad = [theta1.grad.clone(), theta2.grad.clone()]\n",
    "        gradient_contributions.append(sin_term * torch.stack(grad_with_grad))\n",
    "        theta1.grad.zero_()\n",
    "        theta2.grad.zero_()\n",
    "        \n",
    "        # Detached sin term\n",
    "        sin_term_detached = sin_term.detach()\n",
    "        detached_contributions.append(sin_term_detached * cos_term)\n",
    "\n",
    "        # Without detaching\n",
    "        without_detach_contributions.append(sin_term * cos_term)\n",
    "\n",
    "    # Sum contributions for gradients\n",
    "    sum_with_grad = torch.stack([sum([gc[i] for gc in gradient_contributions]) for i in range(2)])\n",
    "\n",
    "    # Sum contributions for detached and without detach\n",
    "    sum_with_detach = sum(detached_contributions)\n",
    "    sum_without_detach = sum(without_detach_contributions)\n",
    "\n",
    "    # Compute gradients\n",
    "    sum_with_detach.backward(retain_graph=True)\n",
    "    detached_grads = [theta1.grad.item(), theta2.grad.item()]\n",
    "    \n",
    "    theta1.grad.zero_()\n",
    "    theta2.grad.zero_()\n",
    "    \n",
    "    sum_without_detach.backward()\n",
    "    without_detach_grads = [theta1.grad.item(), theta2.grad.item()]\n",
    "\n",
    "    # Store results\n",
    "    results_with_grad.append((N, sum_with_grad.detach().numpy()))\n",
    "    results_with_detach.append((N, detached_grads))\n",
    "    results_without_detach.append((N, without_detach_grads))\n",
    "\n",
    "# Print corrected results without in-place operation issues\n",
    "for i, N in enumerate(range(1, 101, 5)):\n",
    "    print(f\"N={N}: With Grad={results_with_grad[i][1]}, With Detach={results_with_detach[i][1]}, Without Detach={results_without_detach[i][1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('fictitious')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3121cab126e9bc214d9c30934815672d7d73cf142023432050561f8ae595f616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
